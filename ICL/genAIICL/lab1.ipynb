{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykySGZSHbFLf"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "ðŸ‘‹ Welcome to the practical sessions of the Generative AI course.\n",
        "\n",
        "These tutorials are designed to complement the lectures by focusing on the implementation of deep generative models.\n",
        "\n",
        "Throughout the semester, we will work with various architectures, from basic Neural Networks to Generative Adversarial Networks (GANs) and Diffusion Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G7tk_bk3L9Q"
      },
      "source": [
        "## ðŸ« Session Format\n",
        "\n",
        "We believe that **\"Talk is cheap, show me the code.\"** Therefore, our tutorials move quickly from theory to practice:\n",
        "\n",
        "1.  **Concept Overview**: We will briefly walk through the key concepts of each section.\n",
        "2.  **Active Coding**: Instead of listening to a line-by-line code explanation, you will dive straight in. After each subsection, you will have **3-5 minutes** to run the provided code. We encourage you to modify the parameters or inputs to see how the code behaves under different conditions.\n",
        "3.  **Challenge & Verify**: At the end of each part, you will tackle a **10-20 minute exercise**. Afterward, we will spend **5 minutes writing the solution live**, allowing you to benchmark your code against ours. Please raise your hand if you get stuck during exercise!\n",
        "4.  **Smart AI Use**: You have access to Imperial's AI tools via [dAIsy](https://www.imperial.ac.uk/admin-services/ict/training-and-resources/daisy/). Please make good use of it. Using GPT to write exercise code for you is meaningless for your learning. Instead, treat AI as a copilot: ask it about unfamiliar APIs or use it to help debug errors you cannot solve on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNL1JcSl3D_Z"
      },
      "source": [
        "## ðŸ’» Computational Environment: Google Colab\n",
        "\n",
        "We use **Google Colab** as our development environment. This platform provides a standardized Python environment and access to necessary computational resources.\n",
        "\n",
        "Training deep learning models is computationally intensive. To ensure reasonable training times during the session, please make sure you are connected to a GPU before running any code:\n",
        "\n",
        "1. Click on **Runtime** in the top menu.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "4. Click **Save**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8GR4PA23OOj"
      },
      "source": [
        "## ðŸ› ï¸ Jupyter Notebook Workflow\n",
        "\n",
        "We use Jupyter Notebooks for interactive coding. Please follow these operational guidelines:\n",
        "\n",
        "1. **Code Execution**\n",
        "\n",
        "- To execute a cell, select it and press Shift + Enter (or click the \"Play\" button â–¶ï¸ on the left of the cell).\n",
        "\n",
        "2. **Sequential Execution**\n",
        "\n",
        "- While the Jupyter environment maintains variable states across cells, please remember that if code in one cell relies on variables defined in a previous one, they must be run in order.\n",
        "- However, most code cells in this tutorial are designed to be **self-contained and executable independently**, so you generally don't need to worry about sequential execution unless specified.\n",
        "\n",
        "You can run the cell below to verify your environment and ensure the GPU is correctly enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JUIsj4VDwdq",
        "outputId": "5e882df8-7606-4f75-eee0-f89ba77e2ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.9.0+cu126\n",
            "âœ… Success! GPU Available: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"âœ… Success! GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"âš ï¸ Warning: GPU not detected. You are running on CPU.\")\n",
        "    print(\"   -> Please go to 'Runtime' -> 'Change runtime type' -> Select 'T4 GPU'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKf3NGkJ0NeS"
      },
      "source": [
        "# Tutorial 1: Basics of Deep Learning\n",
        "\n",
        "The primary objective of this session is to translate the theoretical concepts covered in the lectureâ€”Neural Networks, Loss Functions, and Optimizationâ€”into practical implementation.\n",
        "\n",
        "We will utilize **PyTorch**, the standard framework for modern Generative AI research. By the end of this session, you will have constructed a complete training pipeline, which serves as the foundation for the more complex generative models (such as GANs and Diffusion Models) that we will explore later in the course.\n",
        "\n",
        "> If you are already familiar enough with the use of PyTorch, I recommend you check out some advanced techniques for training large models on this [repo](https://github.com/stas00/ml-engineering/tree/master?tab=readme-ov-file). I believe you will learn something from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BCJRoHb7_-B"
      },
      "source": [
        "## Part 1: Pytorch Essential\n",
        "\n",
        "- **Tensors**: Understanding the fundamental data structure for high-dimensional data.\n",
        "- **Automatic Differentiation (Autograd)**: How PyTorch computes gradients for optimization automatically.\n",
        "- This mechanism forms the foundation of the learning process in all deep learning models.\n",
        "\n",
        "> Adapted from https://docs.pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_1QBFq_P_JN"
      },
      "source": [
        "### **1.1 Initializing a Tensor**\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays and matrices.\n",
        "\n",
        "In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the modelâ€™s parameters.\n",
        "\n",
        "To build deep learning models, we must first understand how to create Tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_6YZIxEQNfE"
      },
      "source": [
        "#### **1.1.1 Directly from Data**\n",
        "\n",
        "Tensor is the fundamental data structure in PyTorch with the critical ability to run on hardware accelerators. Every tensor is defined by three essential attributes that you will constantly monitor while building generative models:\n",
        "\n",
        "- `shape`: The dimensions of the tensor (e.g., the resolution of an image).\n",
        "\n",
        "- `device`: Where the tensor's data is stored (CPU or an accelerator like GPU).\n",
        "\n",
        "- `dtype`: The numerical precision of the data (e.g., float32).\n",
        "\n",
        "When writing deep learning code, most bugs arise from mismatches in these three attributes. Therefore, it is a best practice to check them frequently. You can use the `.to()` method to move a tensor to a different device or change its data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGE7Z3IXQX7j",
        "outputId": "22eb2b50-80f2-4abf-b601-26d9d589c9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor data:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Shape: torch.Size([2, 2])\n",
            "Device: from cpu to cuda:0\n",
            "Data type: from torch.int64 to torch.float32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Creating a tensor from a Python list\n",
        "data = [[1, 2], [3, 4]]\n",
        "data_tensor = torch.tensor(data)\n",
        "\n",
        "# Determining the available device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Moving the tensor to the accelerator and ensuring it uses float32\n",
        "# For more dtype, please refer to https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype\n",
        "new_tensor = data_tensor.to(device=device, dtype=torch.float32)\n",
        "\n",
        "print(f\"Tensor data:\\n{data_tensor}\")\n",
        "print(f\"Shape: {data_tensor.shape}\")\n",
        "print(f\"Device: from {data_tensor.device} to {new_tensor.device}\")\n",
        "print(f\"Data type: from {data_tensor.dtype} to {new_tensor.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bquBl2eTXSy"
      },
      "source": [
        "#### **1.1.2 With Random or Constant Values**\n",
        "\n",
        "In many scenarios, we need to initialize tensors with specific dimensions by providing a shape tuple. This is common when creating initial noise for a generator or setting up bias vectors.\n",
        "\n",
        "- `torch.rand()`: Generates values from a **Uniform distribution** [0,1).\n",
        "- `torch.randn()`: Generates values from a **Standard Normal distribution** (mean 0, variance 1). This is the standard method for initializing noisy latent vectors in diffusion model.\n",
        "- `torch.zeros()` / `torch.ones()`: Fill the tensor entirely with 0s or 1s, often used for initializing weights or masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrpVxhVzQ2fp",
        "outputId": "1652e807-53ba-4dda-af16-81998512b79a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uniform Random:\n",
            "tensor([[0.5568, 0.5304, 0.5529],\n",
            "        [0.0565, 0.5436, 0.1305]])\n",
            "Normal Random (Gaussian):\n",
            "tensor([[ 0.8217, -0.3364, -1.3640],\n",
            "        [-0.2234, -0.6929,  0.1890]])\n",
            "Zeros:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Ones:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "shape = (2, 3)\n",
        "\n",
        "# Initializing with different distributions\n",
        "rand_tensor = torch.rand(shape)\n",
        "randn_tensor = torch.randn(shape)\n",
        "\n",
        "# Initializing with constants\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "\n",
        "print(f\"Uniform Random:\\n{rand_tensor}\")\n",
        "print(f\"Normal Random (Gaussian):\\n{randn_tensor}\")\n",
        "print(f\"Zeros:\\n{zeros_tensor}\")\n",
        "print(f\"Ones:\\n{ones_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJlkW_35QZM5"
      },
      "source": [
        "#### **1.1.3 From Another Tensor**\n",
        "\n",
        "When developing models, you often need to create new tensors that match the properties of an existing one. Instead of manually retrieving the shape, device, or data type, you can use the `_like` suffix.\n",
        "\n",
        "These methods allow you to clone the structural attributes of a reference tensor while filling it with new values. This ensures that your new tensor is automatically compatible with the rest of your computation pipeline, preventing common errors caused by mismatched devices or dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30ZZMIaBQgF7",
        "outputId": "e1fee638-f452-4213-f3bb-ee2b99d94a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference Shape: torch.Size([2, 2]), Device: cpu, Dtype: torch.float32\n",
            "Ones Like:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "Random Like:\n",
            "tensor([[0.3556, 0.2165],\n",
            "        [0.5916, 0.8796]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a reference tensor\n",
        "x_data = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "\n",
        "# Create new tensors based on x_data's attributes (shape, device, dtype)\n",
        "x_ones = torch.ones_like(x_data)\n",
        "x_rand = torch.rand_like(x_data)\n",
        "\n",
        "print(f\"Reference Shape: {x_data.shape}, Device: {x_data.device}, Dtype: {x_data.dtype}\")\n",
        "print(f\"Ones Like:\\n{x_ones}\")\n",
        "print(f\"Random Like:\\n{x_rand}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrXde_9cTh_F"
      },
      "source": [
        "### **1.2 Operations on Tensor**\n",
        "\n",
        "Now that we know how to create tensors, the next step is to perform computations with them.\n",
        "\n",
        "In Generative AI, we rarely work with a single number; instead, we manipulate large grids of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5qcw0dZM-Er"
      },
      "source": [
        "#### **1.2.1 Basic Arithmetic & Broadcasting**\n",
        "\n",
        "PyTorch operations are typically element-wise.\n",
        "When two tensors have the same shape, operations like addition or multiplication are applied to each corresponding pair of elements.\n",
        "\n",
        "**Broadcasting** is the mechanism that allows operations on tensors of different shapes.\n",
        "PyTorch automatically expands the smaller tensor to match the larger one without copying data, provided their dimensions are compatible.\n",
        "Dimensions are compatible when they are either equal or one of them is 1, comparing from the last dimension.\n",
        "\n",
        "Methods with a trailing underscore (e.g., `add_()`) perform in-place operations. These modify the existing tensor's data directly to save memory, rather than creating a new tensor object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9mBHQEA0qyT",
        "outputId": "a1968f92-a105-40ef-dc6b-7e02dfa9b28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5, 7, 9])\n",
            "tensor([ 4, 10, 18])\n",
            "tensor([11, 12, 13])\n",
            "Shapes: torch.Size([3, 2]) + torch.Size([2]) -> torch.Size([3, 2])\n",
            "tensor([[11., 21.],\n",
            "        [11., 21.],\n",
            "        [11., 21.]])\n",
            "tensor([6, 7, 8])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Element-wise operations\n",
        "print(x + y)\n",
        "print(x * y)\n",
        "\n",
        "# Broadcasting a scalar\n",
        "print(x + 10)\n",
        "\n",
        "# Broadcasting a vector (2,) to a matrix (3, 2)\n",
        "m = torch.ones(3, 2)        # Shape: (3, 2)\n",
        "v = torch.tensor([10, 20])  # Shape: (2,)\n",
        "result = m + v\n",
        "\n",
        "print(f\"Shapes: {m.shape} + {v.shape} -> {result.shape}\")\n",
        "print(result)\n",
        "\n",
        "# In-place operation\n",
        "x.add_(5)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgALHsANyLaz"
      },
      "source": [
        "#### **1.2.2 Linear Algebra Operations**\n",
        "\n",
        "Linear algebra is the mathematical foundation of deep learning. While * performs element-wise multiplication, neural networks rely on Matrix Multiplication to propagate data through layers.\n",
        "\n",
        "For standard matrix-matrix or matrix-vector products, we use `torch.matmul()` or the `@` operator. However, in modern Generative AI research, `torch.einsum()` is the preferred tool. It provides a concise way to represent complex operationsâ€”like dot products, outer products, transpositions, and batch matrix multiplicationsâ€”using a single notation.\n",
        "\n",
        "By specifying the input and output dimension labels (e.g., `'ik,kj->ij'`), you can clearly define how dimensions should be multiplied and summed, making the code much more readable than a sequence of transpose, bmm, and view operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKey8YCU2LtH",
        "outputId": "7ab7d95d-01c9-45a4-ad37-b1e10efbe1b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matmul shape: torch.Size([3, 5])\n",
            "Einsum shape: torch.Size([3, 5])\n",
            "Attention scores shape: torch.Size([2, 8, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Standard Matrix Multiplication\n",
        "a = torch.randn(3, 4)\n",
        "b = torch.randn(4, 5)\n",
        "\n",
        "res_matmul = a @ b  # or torch.matmul(a, b)\n",
        "print(f\"Matmul shape: {res_matmul.shape}\")\n",
        "\n",
        "# Einstein Summation (einsum)\n",
        "# 'ik,kj->ij' means: multiply on dimension 'k', result is 'ij'\n",
        "res_einsum = torch.einsum('ik,kj->ij', a, b)\n",
        "print(f\"Einsum shape: {res_einsum.shape}\")\n",
        "\n",
        "# Batch Matrix Multiplication with einsum\n",
        "# Useful for Attention: (Batch, Head, Seq_L, Dim)\n",
        "q = torch.randn(2, 8, 16, 64)\n",
        "k = torch.randn(2, 8, 16, 64)\n",
        "\n",
        "# Transpose and batch multiply: (b, h, i, d) and (b, h, j, d) -> (b, h, i, j)\n",
        "attn_scores = torch.einsum('bhid,bhjd->bhij', q, k)\n",
        "print(f\"Attention scores shape: {attn_scores.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5XWr-snyO1m"
      },
      "source": [
        "#### **1.2.3 Indexing, Slicing & Joining**\n",
        "\n",
        "Manipulating tensors often involves extracting specific sub-regions or merging multiple tensors. Indexing and slicing in PyTorch follow the same conventions as Python and NumPy: you can use `[start:end]` to select ranges across different dimensions.\n",
        "\n",
        "For combining tensors, the two primary functions are `torch.cat()` and `torch.stack()`:\n",
        "\n",
        "`torch.cat()` joins a sequence of tensors along an existing dimension.\n",
        "\n",
        "`torch.stack()` joins tensors along a new dimension, increasing the total number of dimensions by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCzYfb2J4Vj0",
        "outputId": "9014910d-9b02-45e8-e009-24ebcd091d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.1698,  0.2822, -0.9992,  0.9689])\n",
            "tensor([[ 0.2822, -0.9992],\n",
            "        [ 0.2134,  0.2556],\n",
            "        [ 1.1958, -0.2234]])\n",
            "tensor(1.2086)\n",
            "Cat shape: torch.Size([4, 3])\n",
            "Stack shape: torch.Size([2, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(3, 4)\n",
        "\n",
        "# Basic Indexing and Slicing\n",
        "print(x[0, :])      # First row, all columns\n",
        "print(x[:, 1:3])    # All rows, columns 1 and 2\n",
        "print(x[-1, -1])    # Last element\n",
        "\n",
        "# Joining Tensors\n",
        "t1 = torch.zeros(2, 3)\n",
        "t2 = torch.ones(2, 3)\n",
        "\n",
        "# Concatenate along an existing dimension (rows)\n",
        "res_cat = torch.cat([t1, t2], dim=0)\n",
        "print(f\"Cat shape: {res_cat.shape}\")\n",
        "\n",
        "# Stack along a new dimension\n",
        "res_stack = torch.stack([t1, t2], dim=0)\n",
        "print(f\"Stack shape: {res_stack.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBxIIePyRGC"
      },
      "source": [
        "#### **1.2.4 Shape Manipulation**\n",
        "\n",
        "In Generative AI, you frequently transform tensors between different representationsâ€”such as flattening an image into a vector or reordering dimensions for attention mechanisms.\n",
        "\n",
        "PyTorch stores tensor data as a flat array in memory. The shape of a tensor determines how this data is indexed and interpreted.\n",
        "\n",
        "- `.unsqueeze()` and `.squeeze()`: These methods add or remove dimensions of size 1. This is necessary when preparing a single sample to be processed as a batch, as neural network layers typically expect a batch dimension.\n",
        "\n",
        "- `.permute()`: This reorders the dimensions of a tensor. It is often used to switch between different data format conventions, such as moving the channel dimension from the second position to the last.\n",
        "\n",
        "- `.transpose()`: This swaps two specific dimensions of a tensor. It is commonly used for matrix multiplication adjustments or specific axis swapping, unlike `.permute()` which can reorder all dimensions at once.\n",
        "\n",
        "- `.view()` and `.reshape()`: Both methods change the dimensions of a tensor. `.view()` changes the shape without copying the underlying data, which makes it highly efficient. However, it only works if new shape does not change its memory layout. `.reshape()` is more robust; it will return a view if possible, but it will automatically copy the data to a new memory location if the current layout does not support the requested shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul5n-FUA9Coj",
        "outputId": "e52d5f54-8ca5-457a-fd0f-30f13393bf07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single sample: torch.Size([3, 32, 32]) -> Batched: torch.Size([1, 3, 32, 32])\n",
            "Batched: torch.Size([1, 3, 32, 32]) -> Recovered: torch.Size([3, 32, 32])\n",
            "Unsqueezed: torch.Size([1, 4, 1, 3, 32, 32]) -> Squeezed: torch.Size([4, 3, 32, 32])\n",
            "NCHW shape: torch.Size([4, 3, 32, 32]) -> permuted NHWC shape: torch.Size([4, 32, 32, 3])\n",
            "NCHW shape: torch.Size([4, 3, 32, 32]) -> tranposed CNHW shape: torch.Size([3, 4, 32, 32])\n",
            "Original: torch.Size([4, 3, 32, 32]) -> Flattened: torch.Size([4, 3072])\n",
            "!!!ERROR MESSAGE: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "\n",
        "# Adding a batch dimension\n",
        "sample = torch.randn(3, 32, 32)\n",
        "batched_sample = sample.unsqueeze(0)\n",
        "print(f\"Single sample: {sample.shape} -> Batched: {batched_sample.shape}\")\n",
        "\n",
        "# Removing newly added dimension\n",
        "recovered_sample = batched_sample.squeeze(0)\n",
        "print(f\"Batched: {batched_sample.shape} -> Recovered: {recovered_sample.shape}\")\n",
        "\n",
        "# Removing all dimension with size 1 when no specifying dimension\n",
        "unsqueezed_sample = x.unsqueeze(0).unsqueeze(2)\n",
        "squeezed_sample = unsqueezed_sample.squeeze()\n",
        "print(f\"Unsqueezed: {unsqueezed_sample.shape} -> Squeezed: {squeezed_sample.shape}\")\n",
        "\n",
        "# Reordering dimensions (e.g., NCHW to NHWC)\n",
        "permuted = x.permute(0, 2, 3, 1)\n",
        "print(f\"NCHW shape: {x.shape} -> permuted NHWC shape: {permuted.shape}\")\n",
        "\n",
        "# Swap two dimensions (e.g., NCHW to CNHW)\n",
        "transposed = x.transpose(0, 1)\n",
        "print(f\"NCHW shape: {x.shape} -> tranposed CNHW shape: {transposed.shape}\")\n",
        "\n",
        "# Flattening spatial dimensions\n",
        "# The -1 argument calculates the remaining dimension automatically\n",
        "flattened = x.reshape(4, -1)\n",
        "print(f\"Original: {x.shape} -> Flattened: {flattened.shape}\")\n",
        "\n",
        "# view operation could fail when memory layout does not fit the new one\n",
        "x = torch.randn(2, 4, 8)\n",
        "z = x[:, ::2]\n",
        "try:\n",
        "  z1 = z.view(-1)\n",
        "except Exception as e:\n",
        "  print(f\"!!!ERROR MESSAGE: {e}\")\n",
        "print(z.reshape(-1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-WAor27yTYf"
      },
      "source": [
        "#### **1.2.5 Reduction & Statistics**\n",
        "\n",
        "Reduction operations aggregate data in a tensor into a smaller number of elements, often resulting in a single scalar. These operations are fundamental for calculating loss values and performing data normalization.\n",
        "\n",
        "Common reduction functions include `.mean()`, `.sum()`, `.std()`, and `.max()`. The most important parameter for these functions is `dim`, which specifies the dimension along which the reduction is performed. If you do not specify a dimension, the operation reduces the entire tensor to a single value.\n",
        "\n",
        "Another critical parameter is `keepdim=True`. This retains the reduced dimension as size 1 instead of removing it. Maintaining the original number of dimensions is often necessary to ensure the resulting tensor remains compatible for broadcasting in later steps of a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFZxepdCAemL",
        "outputId": "3d58265c-b4c6-4827-84a3-069a3bd9433b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tensor (3x4):\n",
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "Global mean: 5.5, sum: 66.0, std: 3.605551242828369, max: 11.0\n",
            "Sum along dim 0: tensor([12., 15., 18., 21.]) (Shape: torch.Size([4]))\n",
            "Mean along dim 1 (keepdim=True):\n",
            "tensor([[1.5000],\n",
            "        [5.5000],\n",
            "        [9.5000]]) (Shape: torch.Size([3, 1]))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
        "print(f\"Original Tensor (3x4):\\n{x}\")\n",
        "\n",
        "# Reduce the entire tensor\n",
        "print(f\"Global mean: {x.mean()}, sum: {x.sum()}, std: {x.std()}, max: {x.max()}\")\n",
        "\n",
        "# Reduction along a specific dimension\n",
        "# dim=0 reduces across rows (collapses the vertical axis)\n",
        "sum_dim0 = x.sum(dim=0)\n",
        "print(f\"Sum along dim 0: {sum_dim0} (Shape: {sum_dim0.shape})\")\n",
        "\n",
        "# Using keepdim=True\n",
        "# This is useful for broadcasting back to the original shape\n",
        "mean_keepdim = x.mean(dim=1, keepdim=True)\n",
        "print(f\"Mean along dim 1 (keepdim=True):\\n{mean_keepdim} (Shape: {mean_keepdim.shape})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sU4nnaLSfJL"
      },
      "source": [
        "#### **1.2.6 Replication**\n",
        "\n",
        "In Generative AI, you often need to replicate a single vector or a small feature map to match the dimensions of a larger tensor. PyTorch provides two primary methods for this: `.expand()` and `.repeat()`.\n",
        "\n",
        "- `.expand()`: This method increases the size of a tensor by replicating its values across singleton dimensions (dimensions of size 1). It is highly memory-efficient because it does not allocate new memory or copy data; it simply changes the metadata of the tensor to interpret the existing data as being larger.\n",
        "\n",
        "- `.repeat()`: Unlike expand, this method creates a new tensor and physically copies the data a specified number of times along each dimension. This consumes more memory but results in a tensor where the data is explicitly stored in the new shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMDi05PFS_kP",
        "outputId": "0ed3d31d-5c30-4c10-f937-70d4f0320e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: torch.Size([1, 3]) -> Expanded: torch.Size([4, 3])\n",
            "tensor([[1.0000, 0.5000, 0.0000],\n",
            "        [1.0000, 0.5000, 0.0000],\n",
            "        [1.0000, 0.5000, 0.0000],\n",
            "        [1.0000, 0.5000, 0.0000]])\n",
            "Original: torch.Size([1, 3]) -> Repeated: torch.Size([4, 3])\n",
            "tensor([[1.0000, 0.5000, 0.0000],\n",
            "        [1.0000, 0.5000, 0.0000],\n",
            "        [1.0000, 0.5000, 0.0000],\n",
            "        [1.0000, 0.5000, 0.0000]])\n",
            "Original Pointer: 175434048\n",
            "Expanded Pointer: 175434048 (Same as original)\n",
            "Repeated Pointer: 158081344 (New memory address)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# A vector representing a 3-channel color or style\n",
        "x = torch.tensor([[1.0, 0.5, 0.0]]) # Shape: (1, 3)\n",
        "\n",
        "# 1. expand(): Memory-efficient replication\n",
        "# The dimensions must be compatible (starting from size 1)\n",
        "expanded = x.expand(4, 3)\n",
        "print(f\"Original: {x.shape} -> Expanded: {expanded.shape}\")\n",
        "print(expanded)\n",
        "\n",
        "# 2. repeat(): Physical data replication\n",
        "# Arguments specify how many times to repeat along each dimension\n",
        "repeated = x.repeat(4, 1)\n",
        "print(f\"Original: {x.shape} -> Repeated: {repeated.shape}\")\n",
        "print(repeated)\n",
        "\n",
        "# .data_ptr() returns the address of the first element in Tensor\n",
        "print(f\"Original Pointer: {x.data_ptr()}\")\n",
        "print(f\"Expanded Pointer: {expanded.data_ptr()} (Same as original)\")\n",
        "print(f\"Repeated Pointer: {repeated.data_ptr()} (New memory address)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTVv3WdSJfWZ"
      },
      "source": [
        "### **1.3 Autograd: Automatic Differentiation**\n",
        "\n",
        "Autograd is the engine that powers neural network training. It automatically calculates the derivatives required for optimization by building a dynamic **Computational Graph**. Every time you perform an operation on a tensor, PyTorch records it in this graph, allowing it to apply the chain rule and compute gradients efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b1Z9yjwKnPz"
      },
      "source": [
        "#### **1.3.1 Gradient Tracking and Backward Pass**\n",
        "\n",
        "To compute gradients for a specific tensor, you must set `requires_grad=True` during initialization or by calling `x.requires_grad_(True)`. This instruction tells PyTorch to track every subsequent operation involving that tensor.\n",
        "\n",
        "Once you have computed a final scalar valueâ€”typically a lossâ€”you call the `.backward()` method. This triggers a traversal of the computational graph from the output back to the inputs, calculating the gradient of the output with respect to each tracked tensor. These calculated values are stored in the `.grad` attribute of the respective tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeNNnmDoNef-",
        "outputId": "d8382077-c5c1-42cc-ffde-36ddcec313d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients on x: tensor([6., 9.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor and enable gradient tracking\n",
        "# We use float32 as gradients require floating point types\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Define a function: y = 3x^2 + 5\n",
        "y = 3 * x**2 + 5\n",
        "# To compute gradients, the output must be a scalar (e.g., the mean)\n",
        "out = y.mean()\n",
        "\n",
        "# Compute gradients using backpropagation\n",
        "out.backward()\n",
        "\n",
        "# Access the gradient d(out)/dx\n",
        "# For out = (1/2) * sum(3x^2 + 5), d(out)/dx = 3x\n",
        "print(f\"Gradients on x: {x.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN898_y6NxR6"
      },
      "source": [
        "#### **1.3.2 Disabling Gradient Tracking**\n",
        "\n",
        "During model evaluation or inferenceâ€”such as when using a trained GAN to generate imagesâ€”we do not need to calculate gradients. Tracking these operations consumes memory and slows down computation. We use a context manager `torch.no_grad()` to temporarily disable Autograd, which is essential for efficient generation.\n",
        "\n",
        "For Tensor, we can use `.requires_grad` attribute to see if it enables gradient. While for the Python context, we use `torch.is_grad_enabled()`. Even within a context where Autograd enabled, we can create a new tensor which shares the same data but is disconnected from the computational graph using `.detach()`.\n",
        "\n",
        "Sometimes if you want to create a new tensor with autograd relationship to the original one, use `.clone()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcgU_9rPN22H",
        "outputId": "b2bf0cf0-dcd5-4bec-a2e3-a2505ac4758f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking enabled for x? True\n",
            "Tracking enabled in context? True\n",
            "---------- ENTER torch.no_grad() ----------\n",
            "Tracking enabled for y_inference? False\n",
            "Tracking enabled in context? False\n",
            "---------- LEAVE torch.no_grad() ----------\n",
            "Tracking enabled for x? True\n",
            "Tracking enabled in context? True\n",
            "Tracking enabled for y_detached? False\n",
            "Tracking enabled for y_clone? True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor and enable gradient tracking\n",
        "x = torch.randn(2, 2, requires_grad=True)\n",
        "print(f\"Tracking enabled for x? {x.requires_grad}\")\n",
        "print(f\"Tracking enabled in context? {torch.is_grad_enabled()}\")\n",
        "\n",
        "# During inference, we wrap code in a 'no_grad' context manager\n",
        "with torch.no_grad():\n",
        "    print(\"---------- ENTER torch.no_grad() ----------\")\n",
        "    y_inference = x * 2\n",
        "    print(f\"Tracking enabled for y_inference? {y_inference.requires_grad}\")\n",
        "    print(f\"Tracking enabled in context? {torch.is_grad_enabled()}\")\n",
        "    print(\"---------- LEAVE torch.no_grad() ----------\")\n",
        "\n",
        "print(f\"Tracking enabled for x? {x.requires_grad}\")\n",
        "print(f\"Tracking enabled in context? {torch.is_grad_enabled()}\")\n",
        "\n",
        "# Another common method is .detach(), which creates a copy of the tensor\n",
        "# that is no longer linked to the computational graph\n",
        "y_detached = y.detach()\n",
        "print(f\"Tracking enabled for y_detached? {y_detached.requires_grad}\")\n",
        "\n",
        "# .clone() keep autograd relationship\n",
        "y_clone = y.clone()\n",
        "print(f\"Tracking enabled for y_clone? {y_clone.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuJW6IRiRexq"
      },
      "source": [
        "#### **1.3.3 Gradient Accumulation**\n",
        "\n",
        "A critical detail in PyTorch is that gradients are accumulated by default. When you call `.backward()`, the newly calculated gradients are added to whatever is currently stored in the `.grad` attribute rather than replacing it.\n",
        "\n",
        "While this feature is useful for certain advanced techniquesâ€”like training with very large effective batch sizes on limited hardwareâ€”it means that in a standard training loop, you must explicitly clear the gradients before each update (typically using `optimizer.zero_grad()` which we will introduce later). If you forget to do this, the model will attempt to optimize using a sum of gradients from all previous iterations, leading to incorrect updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GprAaCLWR5JB",
        "outputId": "998dadcd-9fe1-418e-f987-9d623102dfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients after first pass: tensor([[ 1.4034,  1.0280],\n",
            "        [-3.3197,  2.0382]])\n",
            "Gradients after second pass (accumulated): tensor([[3.3727, 2.0849],\n",
            "        [7.7007, 6.1927]])\n",
            "Gradients after clearing: tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Gradients after second pass (no accumulation): tensor([[ 1.9694,  1.0568],\n",
            "        [11.0204,  4.1545]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor and enable gradient tracking\n",
        "x = torch.randn(2, 2, requires_grad=True)\n",
        "\n",
        "# Example of gradient accumulation\n",
        "# Re-running backward on the same output\n",
        "out = (3 * x**2 + 5).mean()\n",
        "out.backward()\n",
        "print(f\"Gradients after first pass: {x.grad}\")\n",
        "\n",
        "out = (3 * x**3 + 5).mean()\n",
        "out.backward()\n",
        "print(f\"Gradients after second pass (accumulated): {x.grad}\")\n",
        "\n",
        "# Manually clearing gradients\n",
        "x.grad.zero_()\n",
        "print(f\"Gradients after clearing: {x.grad}\")\n",
        "\n",
        "# backward again for second pass\n",
        "out = (3 * x**3 + 5).mean()\n",
        "out.backward()\n",
        "print(f\"Gradients after second pass (no accumulation): {x.grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWLTu7PwVGdD"
      },
      "source": [
        "### **1.4 Exercise: Manual Gradient Verification**\n",
        "\n",
        "In this exercise, you will simulate a single forward pass of a Linear Layer followed by a Mean Squared Error (MSE) loss. Your goal is to compute the gradient of the loss with respect to the weights manually and then verify that PyTorchâ€™s `.backward()` produces the exact same result.\n",
        "\n",
        "**Scenario**:\n",
        "Consider a simple linear transformation followed by a Mean Squared Error (MSE) loss:\n",
        "1. **Forward Pass**: $y = Wx$\n",
        "2. **Loss**: $L = \\text{mean}((y - t)^2)$\n",
        "\n",
        "**Challenge**:\n",
        "Your goal is to calculate the gradient $\\frac{\\partial L}{\\partial W}$ manually. Mathematically, the gradient is derived as:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{2}{n} (y - t) x^T,\n",
        "$$\n",
        "where $n$ is the number of elements in the output vector $y$.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "1. **Initialization**:\n",
        "    - $x$: a constant vector with values $[1,2,3]$.\n",
        "    - $t$: a vector of size `(2,)` using Uniform distribution $[0, 1)$.\n",
        "    - Move tensors $x$ and $t$ to device `cuda` and dtype `torch.float32`\n",
        "    - $W$: a `2x3` matrix using a Standard Normal distribution with device `cuda` specified and gradient tracking enabled alongside initialization.\n",
        "2. **Standard Autograd Workflow**:\n",
        "    - Compute the predicted output $y$.\n",
        "    - Compute the MSE loss.\n",
        "    - Execute the backward pass to calculate the gradient of $W$.\n",
        "3. **Manual Gradient Calculation**:\n",
        "    - Disable gradient tracking.\n",
        "    - Calculate the error vector $(y - t)$.\n",
        "    - Properly align the dimensions of the difference vector and $x$ to perform a matrix product that results in a `2x3` matrix.\n",
        "    - Account for the scaling factor introduced by the mean operation.\n",
        "\n",
        "The provided verification code at the end of the cell will automatically compare your manual_grad with PyTorch's `W.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3oFR9PsVNZ3",
        "outputId": "1040997b-243d-4353-a011-16fc2f01960b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "PyTorch Gradient:\n",
            "tensor([[ -4.4239,  -8.8477, -13.2716],\n",
            "        [  0.7544,   1.5087,   2.2631]], device='cuda:0')\n",
            "Manual Gradient:\n",
            "tensor([[ -4.4239,  -8.8477, -13.2716],\n",
            "        [  0.7544,   1.5087,   2.2631]], device='cuda:0')\n",
            "Match Found: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "################################################################################\n",
        "# 1. Initialization - TODO\n",
        "#   1) x: size (3), a constant vector with values [1,2,3]\n",
        "#   2) t: size (2), uniform distribution [0,1)\n",
        "#   3) Move tensors $x$ and $t$ to device `cuda` and dtype `torch.float32`\n",
        "#   4) W: a `2x3` matrix using a Standard Normal distribution with device `cuda`\n",
        "#      specified and gradient tracking enabled alongside initialization.\n",
        "################################################################################\n",
        "\n",
        "# TODO: Your code here\n",
        "x = torch.tensor([1, 2, 3]) # (3,)\n",
        "t = torch.rand((2,))\n",
        "x = x.to(device=\"cuda\", dtype=torch.float32)\n",
        "t = t.to(device=\"cuda\", dtype=torch.float32)\n",
        "W = torch.randn((2, 3), device=\"cuda\", requires_grad=True)\n",
        "\n",
        "################################################################################\n",
        "# 2. Standard Autograd Workflow - TODO\n",
        "#   1) Compute y = Wx\n",
        "#   2) Compute loss = mean((y - t)^2)\n",
        "#   3) Trigger the backward pass\n",
        "################################################################################\n",
        "\n",
        "# TODO: Your code here\n",
        "y = W @ x\n",
        "loss = torch.mean((y - t) * (y - t))\n",
        "loss.backward()\n",
        "\n",
        "################################################################################\n",
        "# 3. Manual Gradient Calculation - TODO\n",
        "#   1) Disable gradient tracking\n",
        "#   2) Calculate the error vector (y - t)\n",
        "#   3) Properly align the dimensions of the error vector and input x to\n",
        "#      perform a matrix product that results in a 2x3 matrix.\n",
        "#   4) Account for the scaling factor introduced by the mean operation\n",
        "# HINT: The shape of grad should be the same as the tensor itself!!!\n",
        "################################################################################\n",
        "\n",
        "# TODO: Your code here\n",
        "with torch.no_grad():\n",
        "    error = (y - t)  # (2,)\n",
        "    manual_grad = error.unsqueeze(1) @ x.unsqueeze(0)\n",
        "    manual_grad = manual_grad * 2 / y.shape[0]\n",
        "\n",
        "################################################################################\n",
        "# --- Verification Code (Implemented) ---\n",
        "print(\"-\" * 30)\n",
        "pytorch_grad = W.grad.clone() if (W is not None and W.grad is not None) else None\n",
        "if pytorch_grad is not None and manual_grad is not None:\n",
        "    is_correct = torch.allclose(pytorch_grad, manual_grad)\n",
        "    print(f\"PyTorch Gradient:\\n{pytorch_grad}\")\n",
        "    print(f\"Manual Gradient:\\n{manual_grad}\")\n",
        "    print(f\"Match Found: {is_correct}\")\n",
        "else:\n",
        "    print(\"Please complete the TODO sections to run the verification.\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2LFVrw6LZmU"
      },
      "source": [
        "## Part 2: Building Neural Networks\n",
        "\n",
        "- **Layer**: Learn basic building blocks that frequently used in Generate AI.\n",
        "- **Module**: Learn to wrap layers and forward logic into cascaded structures using the `nn.Module` base class.\n",
        "- A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
        "\n",
        "> Adapted from https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_4-zb_WsY9M"
      },
      "source": [
        "### **2.1 Layers with Parameters**\n",
        "\n",
        "In the previous part, we manually managed weights and biases as raw tensors. In practice, PyTorch provides basic **layers** that automatically initialize and store their own **learnable parameters**.\n",
        "\n",
        "In Generative AI, these parameters represent the **knowledge** the model acquires. When we say we are training a diffusion model, we are actually updating the weight matrices stored inside these layers to better map noise into realistic pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UvtCUJ31ByA"
      },
      "source": [
        "#### **2.1.1 Linear Layers**\n",
        "\n",
        "Linear layers are the simplest building blocks in neural networks, implementing matrix multiplication with an optional bias term. Let's see how to create and use them with `nn.Linear`. The key parameters are `in_features` and `out_features`, with an optional `bias` parameter (defaults to True). The layer works on inputs of any shape, applying the linear transformation to the last dimension only.\n",
        "\n",
        "In PyTorch, each layer has its own default initialization method, but you can also manually initialize weights using the `nn.init` functions. For linear layers, common weight initializations include Xavier, Kaiming, and Normal distributions, while biases are typically initialized to zeros or ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7ztyjVg4_MO",
        "outputId": "9761d42c-e36c-4d6d-d164-952b7059ca65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weight shape: torch.Size([5, 10])\n",
            "Bias shape: torch.Size([5])\n",
            "Input shape: torch.Size([3, 10])\n",
            "Output shape: torch.Size([3, 5])\n",
            "\n",
            "3D input handling:\n",
            "Input shape: torch.Size([2, 3, 10])\n",
            "Output shape: torch.Size([2, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a linear layer: 10 input features â†’ 5 output features\n",
        "linear_layer = nn.Linear(in_features=10, out_features=5)\n",
        "\n",
        "# Check automatically initialized parameters\n",
        "print(\"Weight shape:\", linear_layer.weight.shape)  # (5, 10)\n",
        "print(\"Bias shape:\", linear_layer.bias.shape)      # (5,)\n",
        "\n",
        "# Create input data: batch size=3, features=10\n",
        "x = torch.randn(3, 10)\n",
        "y = linear_layer(x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", y.shape)  # (3, 5)\n",
        "\n",
        "# Linear layers work with multi-dimensional inputs\n",
        "x_3d = torch.randn(2, 3, 10)\n",
        "y_3d = linear_layer(x_3d)\n",
        "print(\"\\n3D input handling:\")\n",
        "print(\"Input shape:\", x_3d.shape)\n",
        "print(\"Output shape:\", y_3d.shape)  # (2, 3, 5)\n",
        "\n",
        "# For more initialization methods, please refer to https://docs.pytorch.org/docs/stable/nn.init.html\n",
        "# Please note the underscore suffix indicates in-place operation\n",
        "xavier_weight = nn.init.xavier_uniform_(linear_layer.weight)  # Xavier initialization\n",
        "kaiming_weight = nn.init.kaiming_uniform_(linear_layer.weight, nonlinearity='relu')  # Kaiming initialization\n",
        "normal_weight = nn.init.normal_(linear_layer.weight, mean=0.0, std=0.01)  # Normal distribution\n",
        "\n",
        "zeros_bias = nn.init.zeros_(linear_layer.bias)\n",
        "ones_bias = nn.init.ones_(linear_layer.bias)  # Sometimes useful to initialize bias to ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXhOW2U01QOS"
      },
      "source": [
        "#### **2.1.2 Convolution Layers**\n",
        "\n",
        "Convolutional layers are essential for processing grid-like data such as images. They apply filters (kernels) that slide across the input to extract local patterns and spatial hierarchies. PyTorch provides `nn.Conv1d`, `nn.Conv2d`, and `nn.Conv3d` for 1D, 2D, and 3D convolutions respectively, with `nn.Conv2d` being the most commonly used for image processing.\n",
        "\n",
        "The key parameters of a convolution layer include `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, and `dilation`. The layer outputs a feature map where each channel corresponds to one filter's response across the input.\n",
        "\n",
        "The spatial dimensions of the output feature map can be calculated using this formula:\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\lfloor \\frac{\\text{Input}+2\\times \\text{Padding}-\\text{Kernel}\\quad}{\\text{Stride}}\\rfloor + 1\n",
        "$$\n",
        "\n",
        "Where $\\lfloor \\cdot \\rfloor$ denotes floor division (rounding down). `padding` adds zeros around the input border, while `stride` determines how many pixels the filter moves each step.\n",
        "\n",
        "> From https://github.com/vdumoulin/conv_arithmetic\n",
        "\n",
        "Here is an animated demonstration showing how different `nn.Conv2d` parameters affect the output, and it assumes that height and width are the same.\n",
        "\n",
        "However, you may encounter cases where the aspect ratio is not 1, and there may even be more than two dimensions (for example `nn.Conv3d`).\n",
        "In such cases these parameters need to be specified separately for each dimension.\n",
        "\n",
        "Note that blue maps are inputs, and cyan maps are outputs.\n",
        "**I, P, K, S, D** are short for input size (in height/width, not channels), `Padding`, `Kernel`, `Stride` and `Dilation`, respectively.\n",
        "\n",
        "\n",
        "<table style=\"width:100%; table-layout:fixed;\">\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/no_padding_no_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/arbitrary_padding_no_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/same_padding_no_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/full_padding_no_strides.gif\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>I=4, P=0, K=3, S=1, D=1</td>\n",
        "    <td>I=5, P=2, K=4, S=1, D=1</td>\n",
        "    <td>I=5, P=1, K=3, S=1, D=1</td>\n",
        "    <td>I=5, P=2, K=3, S=1, D=1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/no_padding_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/padding_strides.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/padding_strides_odd.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/dilation.gif\"</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>I=5, P=0, K=3, S=2, D=1</td>\n",
        "    <td>I=5, P=1, K=3, S=2, D=1</td>\n",
        "    <td>I=6, P=1, K=3, S=2, D=1</td>\n",
        "    <td>I=7, P=0, K=3, S=1, D=2</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9kzMcv6Ketr",
        "outputId": "315776dd-03ae-460c-bb5c-4201475de603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weight shape: torch.Size([16, 8, 3, 3])\n",
            "Bias shape: torch.Size([16])\n",
            "Input shape: torch.Size([2, 8, 4, 4])\n",
            "Output shape: torch.Size([2, 16, 2, 2])\n",
            "\n",
            "Different convolution configurations:\n",
            "Example 2: torch.Size([2, 8, 5, 5]) â†’ torch.Size([2, 16, 6, 6])\n",
            "Example 7: torch.Size([2, 8, 6, 6]) â†’ torch.Size([2, 16, 3, 3])\n",
            "Example 8: torch.Size([2, 8, 7, 7]) â†’ torch.Size([2, 16, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a 2D convolutional layer (Example 1 in illustration)\n",
        "conv_layer = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
        "print(\"Weight shape:\", conv_layer.weight.shape)  # (16, 8, 3, 3)\n",
        "print(\"Bias shape:\", conv_layer.bias.shape)      # (16,)\n",
        "\n",
        "# Create a sample input: batch=2, channels=8, height=4, width=4\n",
        "x = torch.randn(2, 8, 4, 4)\n",
        "y = conv_layer(x)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", y.shape)  # (2, 16, 2, 2)\n",
        "\n",
        "# Demonstrating different parameter configurations\n",
        "print(\"\\nDifferent convolution configurations:\")\n",
        "\n",
        "# Example 2\n",
        "conv1 = nn.Conv2d(8, 16, kernel_size=4, stride=1, padding=2)\n",
        "x1 = torch.randn(2, 8, 5, 5)\n",
        "y1 = conv1(x1)\n",
        "print(f\"Example 2: {x1.shape} â†’ {y1.shape}\")  # (2, 16, 6, 6)\n",
        "\n",
        "# Example 7\n",
        "conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1)\n",
        "x2 = torch.randn(2, 8, 6, 6)\n",
        "y2 = conv2(x2)\n",
        "print(f\"Example 7: {x2.shape} â†’ {y2.shape}\")  # (2, 16, 3, 3)\n",
        "\n",
        "# Example 8\n",
        "conv3 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=0, dilation=2)\n",
        "x3 = torch.randn(2, 8, 7, 7)\n",
        "y3 = conv3(x3)\n",
        "print(f\"Example 8: {x3.shape} â†’ {y3.shape}\")  # (2, 16, 3, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVGH8Sbn2ISq"
      },
      "source": [
        "#### **2.1.3 Normalization Layers**\n",
        "\n",
        "Normalization layers stabilize and accelerate neural network training by normalizing activations. Different normalization methods operate on different dimensions of the input tensor, each suited for specific architectures and tasks.\n",
        "\n",
        "Consider an input tensor of shape $(N, C, H, W)$, where each dimension represents batch, channel, height and width, respectively.\n",
        "\n",
        "There are currently four commonly used normalization methods:\n",
        "\n",
        "- **BatchNorm**: Normalizes across the batch dimension $(N)$ and spatial dimensions $(H, W)$ for each channel $(C)$. It computes one mean and variance per channel across all samples in the batch.\n",
        "\n",
        "- **LayerNorm**: Normalizes across the channel dimension $(C)$ and spatial dimensions $(H, W)$ for each sample $(N)$. It computes one mean and variance per sample across all channels.\n",
        "\n",
        "- **InstanceNorm**: Normalizes across the spatial dimensions $(H, W)$ for each channel $(C)$ of each sample $(N)$. It computes separate statistics for each channel in each sample.\n",
        "\n",
        "- **GroupNorm**: Divides channels into groups and normalizes across the group dimension and spatial dimensions $(H, W)$ for each sample $(N)$. It's a compromise between LayerNorm and InstanceNorm.\n",
        "\n",
        "The illustration below shows the different combinations of these dimensions. You may need to look carefully at the subtle differences between them.\n",
        "\n",
        "> From https://github.com/facebookresearch/Detectron/tree/main/projects/GN\n",
        "\n",
        "<img width=\"100%\" src=\"https://raw.githubusercontent.com/facebookresearch/Detectron/refs/heads/main/projects/GN/gn.jpg\">\n",
        "\n",
        "In PyTorch, these normalization layers are implemented with specific ways to define the normalization scope:\n",
        "\n",
        "- `nn.BatchNorm` and `nn.InstanceNorm`: Use 1D, 2D, or 3D variants (`BatchNorm1d/2d/3d`, `InstanceNorm1d/2d/3d`) to handle different data types. The number indicates how many spatial dimensions are included in the normalization (like 1D for sequences, 2D for images, 3D for videos). The `num_features` parameter specifies the number of channels.\n",
        "\n",
        "- `nn.LayerNorm`: Requires you to explicitly specify the shape of dimensions to normalize via the `normalized_shape` parameter. This can be a single integer (normalizing just the last dimension) or a tuple (normalizing multiple trailing dimensions).\n",
        "    - Plese note that this API can fairly support various normalization ways and is not limited to the paradigm ($C\\times H\\times W$) as indicated in the figure above.\n",
        "\n",
        "- `nn.GroupNorm`: Requires both `num_groups` (how many groups to divide channels into) and `num_channels` (total number of channels). The layer automatically splits channels into groups and normalizes within each group.\n",
        "\n",
        "All these layers include learnable scale (weight $\\gamma$) and shift (bias $\\beta$) parameters by specifying `affine=True` at channel dimension (except `elementwise_affine` in `nn.LayerNorm` at per-element):\n",
        "$$\n",
        "y=\\frac{x-\\text{E}[x]}{\\surd (\\text{Var}[x]+\\epsilon)}*\\gamma+\\beta\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNFSQAIrUiPv",
        "outputId": "27e05180-e5f1-4960-870f-e07c9c823173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BatchNorm:\n",
            "  running_mean shape: torch.Size([8])\n",
            "  running_var shape: torch.Size([8])\n",
            "  weight shape: torch.Size([8])\n",
            "  bias shape: torch.Size([8])\n",
            "\n",
            "LayerNorm:\n",
            "  weight shape: torch.Size([8, 16, 16])\n",
            "  bias shape: torch.Size([8, 16, 16])\n",
            "\n",
            "InstanceNorm:\n",
            "  weight shape: torch.Size([8])\n",
            "  bias shape: torch.Size([8])\n",
            "\n",
            "GroupNorm (4 groups):\n",
            "  weight shape: torch.Size([8])\n",
            "  bias shape: torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a sample input: batch=4, channels=8, height=16, width=16\n",
        "x = torch.randn(4, 8, 16, 16)\n",
        "\n",
        "# 1. Batch Normalization (nn.BatchNorm)\n",
        "# Normalizes across N, H, W dimensions for each C\n",
        "# Important notes: Since training has the concept of batches while inference does not,\n",
        "#   BatchNorm uses the mean and variance computed from the current training batch during training,\n",
        "#   while maintains a running average for use in inference.\n",
        "batch_norm = nn.BatchNorm2d(num_features=8, affine=True)\n",
        "y_bn = batch_norm(x)\n",
        "print(\"BatchNorm:\")\n",
        "print(f\"  running_mean shape: {batch_norm.running_mean.shape}\")  # (C,)\n",
        "print(f\"  running_var shape: {batch_norm.running_var.shape}\")    # (C,)\n",
        "print(f\"  weight shape: {batch_norm.weight.shape}\")  # (C,)\n",
        "print(f\"  bias shape: {batch_norm.bias.shape}\")      # (C,)\n",
        "\n",
        "# 2. Layer Normalization (LayerNorm)\n",
        "# Normalizes across C, H, W dimensions for each N\n",
        "layer_norm = nn.LayerNorm([8, 16, 16], elementwise_affine=True)  # Normalize last 3 dimensions\n",
        "y_ln = layer_norm(x)\n",
        "print(\"\\nLayerNorm:\")\n",
        "print(f\"  weight shape: {layer_norm.weight.shape}\")  # (C, H, W) -> (8, 16, 16)\n",
        "print(f\"  bias shape: {layer_norm.bias.shape}\")      # (C, H, W) -> (8, 16, 16)\n",
        "\n",
        "# 3. Instance Normalization (nn.InstanceNorm)\n",
        "# Normalizes across H, W dimensions for each C in each N\n",
        "instance_norm = nn.InstanceNorm2d(num_features=8, affine=True)\n",
        "y_in = instance_norm(x)\n",
        "print(\"\\nInstanceNorm:\")\n",
        "print(f\"  weight shape: {instance_norm.weight.shape}\")  # (C,) -> (8,)\n",
        "print(f\"  bias shape: {instance_norm.bias.shape}\")      # (C,) -> (8,)\n",
        "\n",
        "# 4. Group Normalization (GroupNorm)\n",
        "# Divides C into groups, normalizes across group and H, W for each N\n",
        "group_norm = nn.GroupNorm(num_groups=4, num_channels=8, affine=True)  # 8 channels Ã· 4 groups = 2 channels/group\n",
        "y_gn = group_norm(x)\n",
        "print(\"\\nGroupNorm (4 groups):\")\n",
        "print(f\"  weight shape: {group_norm.weight.shape}\")  # (C,) -> (8,)\n",
        "print(f\"  bias shape: {group_norm.bias.shape}\")      # (C,) -> (8,)\n",
        "\n",
        "# Output shape will not change for normalization!\n",
        "# Other types of layer normalization:\n",
        "layer_norm_at_spatial = nn.LayerNorm(16 * 16, elementwise_affine=True)\n",
        "_ = layer_norm_at_spatial(x.reshape(4, 8, -1))  # (4, 8, 16, 16) -> (4, 8, 256)\n",
        "layer_norm_at_channel = nn.LayerNorm(8, elementwise_affine=True)\n",
        "_ = layer_norm_at_channel(x.reshape(4, 8, -1).transpose(1, 2)) # (4, 8, 16, 16) -> (4, 8, 256) -> (4, 256, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_wI5jS2tkkN"
      },
      "source": [
        "### **2.2 Layers without Parameter**\n",
        "\n",
        "In the previous sections, we focused on layers that contain learnable parameters such as weights and biases. However, not all layers in a neural network need to learn from data. Some layers perform fixed operations that are crucial for network performance and stability.\n",
        "\n",
        "These parameter-free layers serve various purposes: transforming spatial dimensions, introducing non-linearities and preventing overfitting. Unlike parameterized layers, they don't store any state that gets updated during training, but they play essential roles in shaping the information flow through the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8IjlXzN2RKY"
      },
      "source": [
        "#### **2.2.1 Pooling Layers**\n",
        "\n",
        "Pooling layers are used to reduce the spatial dimensions of feature maps while preserving the most important information. They help decrease computational complexity and provide basic translation invariance. Unlike convolutional layers which use learned filters to extract features, pooling layers apply fixed operations like taking the maximum or average within sliding windows. In this section we also introduce the reverse operation of pooling, namely upsampling.\n",
        "\n",
        "PyTorch offers several types of pooling-related operations, where their hyperparameter settings are similar with convolutional layers:\n",
        "\n",
        "- `nn.MaxPool1d/2d/3d`: Takes the maximum value in each window, preserving the most prominent features\n",
        "\n",
        "- `nn.AvgPool1d/2d/3d`: Takes the average value in each window, providing smoother downsampling\n",
        "\n",
        "- `nn.AdaptiveMaxPool1d/2d/3d` & `nn.AdaptiveAvgPool1d/2d/3d`: Automatically adjusts pooling to produce a fixed output size regardless of input dimensions, which is particularly useful when dealing with variable-sized inputs\n",
        "\n",
        "- `nn.Upsample`: Increases spatial dimensions through interpolation, sometimes used in generative models for decoding from latent representations to pixel space\n",
        "\n",
        "These layers have no learnable parameters - they simply apply fixed mathematical operations over the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etDeRTfs798G",
        "outputId": "613c1fdf-44ac-4517-aa0b-9c31c3fd682d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nn.MaxPool2d(kernel_size=2, stride=2):\n",
            "  torch.Size([2, 3, 8, 8]) â†’ torch.Size([2, 3, 4, 4])\n",
            "  Operation: For each 2x2 window, output the maximum value\n",
            "\n",
            "nn.AvgPool2d(kernel_size=2, stride=2):\n",
            "  torch.Size([2, 3, 8, 8]) â†’ torch.Size([2, 3, 4, 4])\n",
            "  Operation: For each 2x2 window, output the average value\n",
            "\n",
            "============================================================\n",
            "Adaptive Pooling (guaranteed output size):\n",
            "nn.AdaptiveAvgPool2d(output_size=(4, 4)):\n",
            "  torch.Size([2, 3, 8, 8]) â†’ torch.Size([2, 3, 4, 4])\n",
            "  Use case: Ensures fixed-size output for variable input sizes\n",
            "\n",
            "nn.AdaptiveAvgPool2d(output_size=(1, 1)) - Global Average Pooling:\n",
            "  torch.Size([2, 3, 8, 8]) â†’ torch.Size([2, 3, 1, 1])\n",
            "  Use case: Reduces each feature map to a single value\n",
            "\n",
            "============================================================\n",
            "Upsample/Interpolation (increasing dimensions):\n",
            "nn.Upsample(scale_factor=2, mode='bilinear'):\n",
            "  torch.Size([2, 3, 8, 8]) â†’ torch.Size([2, 3, 16, 16])\n",
            "  Use case: Decoding from latent space to pixel space in generative models\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a sample input: batch=2, channels=3, height=8, width=8\n",
        "x = torch.randn(2, 3, 8, 8)\n",
        "\n",
        "# 1. Basic MaxPooling and AvgPooling (similar to convolution but with fixed operations)\n",
        "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "y_max = max_pool(x)\n",
        "y_avg = avg_pool(x)\n",
        "\n",
        "print(\"nn.MaxPool2d(kernel_size=2, stride=2):\")\n",
        "print(f\"  {x.shape} â†’ {y_max.shape}\")\n",
        "print(\"  Operation: For each 2x2 window, output the maximum value\")\n",
        "\n",
        "print(\"\\nnn.AvgPool2d(kernel_size=2, stride=2):\")\n",
        "print(f\"  {x.shape} â†’ {y_avg.shape}\")\n",
        "print(\"  Operation: For each 2x2 window, output the average value\")\n",
        "\n",
        "# 2. Adaptive Pooling - produces fixed output size regardless of input size\n",
        "# Useful for handling inputs of varying dimensions\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Adaptive Pooling (guaranteed output size):\")\n",
        "\n",
        "# Adaptive Average Pooling to 4x4 output\n",
        "adaptive_avg_pool = nn.AdaptiveAvgPool2d(output_size=(4, 4))\n",
        "y_adaptive_avg = adaptive_avg_pool(x)\n",
        "print(f\"nn.AdaptiveAvgPool2d(output_size=(4, 4)):\")\n",
        "print(f\"  {x.shape} â†’ {y_adaptive_avg.shape}\")\n",
        "print(\"  Use case: Ensures fixed-size output for variable input sizes\")\n",
        "\n",
        "# Global Average Pooling (to 1x1 output) - often used before fully connected layers\n",
        "global_avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "y_global_avg = global_avg_pool(x)\n",
        "print(f\"\\nnn.AdaptiveAvgPool2d(output_size=(1, 1)) - Global Average Pooling:\")\n",
        "print(f\"  {x.shape} â†’ {y_global_avg.shape}\")\n",
        "print(\"  Use case: Reduces each feature map to a single value\")\n",
        "\n",
        "# 3. Upsample/Interpolate - increasing spatial dimensions\n",
        "# Commonly used in generative models and decoder networks\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Upsample/Interpolation (increasing dimensions):\")\n",
        "\n",
        "# Upsample by a scale factor\n",
        "# scale_factor indicates multiplier for spatial size, support 1D/2D/3D at all\n",
        "# mode = ['nearest', 'bilinear', 'bicubic']\n",
        "upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "y_upsample = upsample(x)\n",
        "print(f\"nn.Upsample(scale_factor=2, mode='bilinear'):\")\n",
        "print(f\"  {x.shape} â†’ {y_upsample.shape}\")\n",
        "print(\"  Use case: Decoding from latent space to pixel space in generative models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVvRBQ1r2Vw_"
      },
      "source": [
        "#### **2.2.2 Non-linear Activations**\n",
        "\n",
        "Non-linear activation functions are essential components in neural networks that introduce non-linear transformations, allowing networks to learn complex patterns and relationships. Without activation functions, multiple linear layers would collapse into a single linear transformation, severely limiting the network's representational power.\n",
        "\n",
        "PyTorch provides a comprehensive collection of activation functions, all of which are parameter-free operations. These functions are typically applied element-wise to the outputs of linear or convolutional layers.\n",
        "\n",
        "PyTorch provides a variety of activation functions in ``torch.nn``, each implemented as a parameter-free module. Here are some commonly used ones:\n",
        "\n",
        "- `nn.ReLU` implements the Rectified Linear Unit defined as $f(x) = \\max(0, x)$.\n",
        "\n",
        "- `nn.Sigmoid` implements the Sigmoid function defined as $f(x) = \\frac{1}{1 + e^{-x}}$.\n",
        "\n",
        "- `nn.Tanh` implements the hyperbolic tangent function defined as $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$.\n",
        "\n",
        "- `nn.LeakyReLU` implements the Leaky Rectified Linear Unit defined as $$\n",
        "f(x) = \\begin{cases}\n",
        "x & \\text{if } x \\geq 0 \\\\\n",
        "\\alpha x & \\text{if } x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "- `nn.GELU` implements the Gaussian Error Linear Unit defined as $f(x) = x \\cdot \\Phi(x)$ where $\\Phi(x)$ is the cumulative distribution function of the Gaussian distribution, approximated as $f(x)=0.5 * x * (1 + \\text{Tanh}(\\sqrt{2/\\pi} * (x+0.044715 * x^3)))$\n",
        "\n",
        "- `nn.SiLU` (also known as Swish) implements the Sigmoid Linear Unit defined as $f(x) = x \\cdot \\text{sigmoid}(x)$.\n",
        "\n",
        "Their formulas are all listed, of which only `nn.LeakyReLU` has a hyperparameter $\\alpha$, and the others contain no hyperparameters.\n",
        "The choice of activation functions in generative models is primarily an engineering consideration that requires extensive experiments through ablation studies, as there is no universal solution suitable to all scenarios.\n",
        "\n",
        "Below, I have plotted these six activation functions through code for easy comparison. The visualization code uses the `matplotlib` library, which requires input data to be in `numpy` format. `Numpy` arrays can easily be converted using `tensor.numpy()`, but be careful that the tensor must be on the CPU and not on the GPU.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "h3i4ORYEN3hn",
        "outputId": "02b53cbb-0de2-4b3b-c258-29e3ba297554"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create sample data\n",
        "x = torch.linspace(-3, 3, 200)\n",
        "\n",
        "# Define activation functions\n",
        "activations = {\n",
        "    'ReLU': nn.ReLU(),\n",
        "    'Sigmoid': nn.Sigmoid(),\n",
        "    'Tanh': nn.Tanh(),\n",
        "    'LeakyReLU(0.1)': nn.LeakyReLU(0.1),\n",
        "    'GELU': nn.GELU(),\n",
        "    'SiLU': nn.SiLU()\n",
        "}\n",
        "\n",
        "# Plot all activation functions together\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Plot each activation function with different colors and line styles\n",
        "colors = plt.cm.Set1(np.linspace(0, 1, len(activations)))\n",
        "line_styles = ['-', '--', '-.', ':', '-', '--']\n",
        "\n",
        "for idx, (name, activation) in enumerate(activations.items()):\n",
        "    y = activation(x)\n",
        "    plt.plot(x.numpy(), y.numpy(),\n",
        "             label=name,\n",
        "             color=colors[idx],\n",
        "             linestyle=line_styles[idx],\n",
        "             linewidth=2)\n",
        "\n",
        "# Configure plot\n",
        "plt.xlabel('x', fontsize=12)\n",
        "plt.ylabel('f(x)', fontsize=12)\n",
        "plt.title('Common Activation Functions in PyTorch', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfU159TS24Em"
      },
      "source": [
        "#### **2.2.3 Dropout Layers**\n",
        "\n",
        "Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero during training. This forces the network to learn more robust features that are not reliant on specific connections between neurons. In PyTorch, dropout is primarily implemented as `nn.Dropout`.\n",
        "\n",
        "The key characteristic of dropout layers is their different behavior during training and evaluation:\n",
        "\n",
        "- **Training mode**: Randomly zeros elements with probability p\n",
        "\n",
        "- **Evaluation mode**: Passes input unchanged (no dropout applied)\n",
        "\n",
        "Furthermore, outputs are scaled by $1/(1-p)$ to maintain the expected value during training.\n",
        "\n",
        "It is worth noting that the training mode and evaluation mode are switched by calling `.train()` and `.eval()` on the module, and the current state can be obtained through `.training` as an attribute.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFvddUjeRMZO",
        "outputId": "6e9e1f7d-669f-4cb9-b08e-261a0bc06a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 10, 8, 8])\n",
            "\n",
            "nn.Dropout(p=0.3):\n",
            "  Input shape: torch.Size([4, 640])\n",
            "  Output shape: torch.Size([4, 640])\n",
            "  Zeros ratio: 0.298 (target: 0.3)\n",
            "\n",
            "Training vs Evaluation mode comparison:\n",
            "  Training mode - zeros: 2/10\n",
            "  Evaluation mode - zeros: 0/10\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create sample data\n",
        "x = torch.randn(4, 10, 8, 8)  # batch=4, channels=10, height=8, width=8\n",
        "print(\"Input shape:\", x.shape)\n",
        "\n",
        "# 1. Basic Dropout\n",
        "dropout = nn.Dropout(p=0.3)\n",
        "x_flat = x.view(4, -1)  # Flatten for 1D dropout\n",
        "y = dropout(x_flat)\n",
        "print(\"\\nnn.Dropout(p=0.3):\")\n",
        "print(f\"  Input shape: {x_flat.shape}\")\n",
        "print(f\"  Output shape: {y.shape}\")\n",
        "print(f\"  Zeros ratio: {(y == 0).float().mean():.3f} (target: 0.3)\")\n",
        "\n",
        "# 2. Training vs Evaluation mode\n",
        "test_input = torch.ones(1, 10)\n",
        "print(\"\\nTraining vs Evaluation mode comparison:\")\n",
        "\n",
        "# Training mode\n",
        "dropout.train()\n",
        "train_output = dropout(test_input.clone())\n",
        "print(f\"  Training mode - zeros: {(train_output == 0).sum().item()}/10\")\n",
        "\n",
        "# Evaluation mode\n",
        "dropout.eval()\n",
        "eval_output = dropout(test_input.clone())\n",
        "print(f\"  Evaluation mode - zeros: {(eval_output == 0).sum().item()}/10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKxhOYrVun8f"
      },
      "source": [
        "### **2.3 Functional Operations**\n",
        "\n",
        "In the previous sections, we covered many layersâ€”both those with parameters and those without. In fact, each of them has a functional version.\n",
        "For layers with parameters, the functional form helps when you need manual control over learnable parameters. For layers without parameters, it offers more flexibility in complex forward logic.\n",
        "\n",
        "In this section, we'll also highlight operations commonly used in generative models that are typically called as functions, not initialized as layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t2e2EcSpQ_"
      },
      "source": [
        "#### **2.3.1 Functional Version of Previous Layers**\n",
        "\n",
        "All layers we've covered have functional equivalents in `torch.nn.functional` (typically imported as `F`). The key distinction is state management: modules store parameters internally, while functional forms accept them as arguments, giving you explicit control.\n",
        "\n",
        "For **parameterized layers**, this is essential for weight sharing and dynamic architectures:\n",
        "\n",
        "- `nn.Linear` â†’ `F.linear(input, weight, bias)`\n",
        "- `nn.Conv1d/2d/3d` â†’ `F.conv1d/2d/3d(input, weight, bias, ...)`\n",
        "- `nn.BatchNorm` â†’ `F.batch_norm(input, running_mean, running_var, ...)` (you manage statistics)\n",
        "- `nn.LayerNorm/InstanceNorm/GroupNorm` â†’ `F.layer_norm/instance_norm/group_norm(input, weight=..., bias=...)` (statistics computed per-input)\n",
        "\n",
        "For **non-parameterized layers**, functional forms offer flexibility in complex forward logic:\n",
        "\n",
        "- Pooling: `nn.AvgPool2d` â†’ `F.avg_pool2d`; `nn.MaxPool2d` â†’ `F.max_pool2d`; `nn.AdaptiveAvgPool2d` â†’ `F.adaptive_avg_pool2d`\n",
        "- Upsampling: `nn.Upsample` â†’ `F.interpolate(input, size, mode='bilinear')` (core for diffusion decoders)\n",
        "- Activations: `nn.ReLU/GELU/SiLU` â†’ `F.relu/gelu/silu(input, inplace=False)`\n",
        "- Dropout: `nn.Dropout` â†’ `F.dropout(input, p, training=self.training)` (must pass training flag explicitly)\n",
        "\n",
        "This direct interface is crucial for research prototypes and advanced generative models. We won't go into much detail on this part. It would be enough for you to know having such a flexible option. When you want to use it, just look up how to use the API from official website (https://docs.pytorch.org/docs/stable/nn.functional.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnkZe8hBbVRk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Input: batch=2, channels=3, height=4, width=4\n",
        "x = torch.randn(2, 3, 4, 4)\n",
        "\n",
        "# Linear: weight(out_features, in_features), bias(out_features)\n",
        "weight, bias = torch.randn(5, 48), torch.randn(5)\n",
        "out_linear = F.linear(x.view(2, -1), weight, bias)\n",
        "\n",
        "# Conv2d: weight(out_channels, in_channels, kH, kW)\n",
        "weight, bias = torch.randn(5, 3, 3, 3), torch.randn(5)\n",
        "out_conv = F.conv2d(x, weight, bias, padding=1)\n",
        "\n",
        "# BatchNorm: pass running_mean, running_var explicitly\n",
        "weight, bias = torch.randn(5), torch.randn(5)\n",
        "running_mean, running_var = torch.zeros(5), torch.ones(5)\n",
        "out_bn = F.batch_norm(out_conv, running_mean, running_var, training=True, weight=weight, bias=bias)\n",
        "\n",
        "# Layer/Instance/Group Norm: compute stats per-input\n",
        "weight, bias = torch.randn(3, 4, 4), torch.randn(3, 4, 4)\n",
        "out_ln = F.layer_norm(x, normalized_shape=[3, 4, 4], weight=weight, bias=bias)\n",
        "\n",
        "weight, bias = torch.randn(3), torch.randn(3)\n",
        "out_in = F.instance_norm(x, weight=weight, bias=bias)\n",
        "out_gn = F.group_norm(x, num_groups=1, weight=weight, bias=bias)\n",
        "\n",
        "# Pooling: stateless operations\n",
        "out_avg = F.avg_pool2d(x, kernel_size=2)\n",
        "out_max = F.max_pool2d(x, kernel_size=2)\n",
        "out_adaptive = F.adaptive_avg_pool2d(x, output_size=1)\n",
        "\n",
        "# Upsampling: core for diffusion decoders\n",
        "out_up = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "\n",
        "# Activations: element-wise transforms\n",
        "out_relu = F.relu(x)\n",
        "out_gelu = F.gelu(x)\n",
        "out_silu = F.silu(x)\n",
        "\n",
        "# Dropout: MUST pass training flag explicitly!\n",
        "out_dropout = F.dropout(x, p=0.5, training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJvNVRG4TG0y"
      },
      "source": [
        "#### **2.3.2 Frequently Used Operations**\n",
        "\n",
        "Now let's look at some operations that you'll call directly as functions rather than initializing as layers. These are building blocks you'll use repeatedly when writing custom forward passes.\n",
        "\n",
        "**Attention Mechanisms**\n",
        "\n",
        "First, let's cover the attention operations that are fundamental to many modern architectures.\n",
        "\n",
        "- `F.softmax` normalizes input along a specified dimension:\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}.\n",
        "$$\n",
        "Usually, before using this method, we divide the logits by a temperature to control how sharp or smooth the distribution becomes.\n",
        "\n",
        "- `F.scaled_dot_product_attention` computes the core attention operation: it takes `query`, `key`, and `value` tensors, plus an optional `attn_mask`. Mathematically, it performs:\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "The function automatically handles scaling, masking, and the weighted sum. It returns the attention output and optionally the attention weights.\n",
        "    - The `attn_mask` argument can be used to prevent attention to certain positions by setting those entries to `-inf` before the softmax.\n",
        "    - For autoregressive tasks (like GPT), you can instead set the boolean `is_causal=True`, which automatically applies a triangular mask to prevent attending to future positions and is typically more memory-efficient than passing a manual tensor.\n",
        "\n",
        "**Loss Functions**\n",
        "\n",
        "Next, we'll look at the loss functions that define how models learn.\n",
        "\n",
        "- `F.binary_cross_entropy_with_logits` computes:\n",
        "$$\n",
        "\\text{loss} = -[y*\\log(\\sigma(x)) + (1-y)*\\log(1-\\sigma(x))],\n",
        "$$\n",
        "where $\\sigma$ is the `sigmoid` function. It takes `input` (raw logits) and `target` tensors.\n",
        "\n",
        "- `F.mse_loss` calculates mean squared error:\n",
        "$$\n",
        "\\text{loss} = \\frac{1}{n}\\sum(y_i - \\hat{y_i})^2.\n",
        "$$\n",
        "It expects `input` and `target` tensors, with a `reduction` parameter to sum or average the result.\n",
        "\n",
        "- `F.kl_div` computes KL divergence:\n",
        "$$\n",
        "\\text{loss} = \\sum y_i * (\\log y_i - x_i).\n",
        "$$\n",
        "It requires `input` as log-probabilities and `target` as probabilities. The `log_target` parameter lets you provide both in log-space for numerical stability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCli3JWBmi1M",
        "outputId": "b4d383cb-dc93-4dff-a1d8-a23fadfca188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results match: True\n",
            "\n",
            "default MSE loss reduction (mean): 2.1463890075683594\n",
            "MSE loss reduction in sum: 206.0533447265625\n",
            "\n",
            "KL loss shape with reduction none: torch.Size([2, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Setup: batch=2, seq_len=10, feature_dim=64\n",
        "Q = torch.randn(2, 10, 64)\n",
        "K = torch.randn(2, 10, 64)\n",
        "V = torch.randn(2, 10, 64)\n",
        "\n",
        "# 1. Scaled Dot-Product Attention\n",
        "attn_out = F.scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Option A: Manual mask (-inf values are ignored/masked)\n",
        "mask = torch.zeros(10, 10)\n",
        "mask.masked_fill_(torch.triu(torch.ones(10, 10, dtype=torch.bool), diagonal=1), float('-inf'))\n",
        "attn_out_masked = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n",
        "\n",
        "# Option B: is_causal=True (Equivalent to Option A, but optimized)\n",
        "attn_out_causal = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
        "\n",
        "# Verify both methods produce the same result\n",
        "print(f\"Results match: {torch.allclose(attn_out_masked, attn_out_causal)}\")\n",
        "\n",
        "# 2. Softmax with temperature\n",
        "logits = torch.randn(2, 10)\n",
        "\n",
        "# Standard softmax\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Temperature scaling: lower temp = sharper distribution\n",
        "sharp_probs = F.softmax(logits / 0.5, dim=-1)\n",
        "smooth_probs = F.softmax(logits / 2.0, dim=-1)\n",
        "\n",
        "# 3. Binary Cross-Entropy with Logits (binary classification)\n",
        "# Input: raw logits, Target: 0 or 1 (must be in [0,1])\n",
        "logits = torch.randn(2, 1)  # predictions for 2 samples\n",
        "targets = torch.tensor([[1.0], [0.0]])  # 1=positive class, 0=negative class\n",
        "\n",
        "bce_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
        "\n",
        "# 4. MSE Loss (reconstruction error)\n",
        "predictions = torch.randn(2, 3, 4, 4)\n",
        "ground_truth = torch.randn(2, 3, 4, 4)\n",
        "\n",
        "mse = F.mse_loss(predictions, ground_truth)\n",
        "print(f\"\\ndefault MSE loss reduction (mean): {mse}\")\n",
        "\n",
        "# With sum reduction (instead of mean) for custom weighting\n",
        "mse_sum = F.mse_loss(predictions, ground_truth, reduction='sum')\n",
        "print(f\"MSE loss reduction in sum: {mse_sum}\")\n",
        "\n",
        "# 5. KL Divergence\n",
        "# Input: log-probabilities, Target: probabilities\n",
        "log_p = torch.log_softmax(torch.randn(2, 10), dim=-1)\n",
        "logit_q = torch.randn(2, 10)\n",
        "q = torch.softmax(logit_q, dim=-1)\n",
        "\n",
        "# reduction could be none to get raw results for further statistics\n",
        "kl_loss = F.kl_div(log_p, q, reduction='none')\n",
        "print(f\"\\nKL loss shape with reduction none: {kl_loss.shape}\")\n",
        "\n",
        "# With log_target for numerical stability\n",
        "log_q = torch.log_softmax(logit_q, dim=-1)\n",
        "kl_log = F.kl_div(log_p, log_q, log_target=True, reduction='none')\n",
        "assert torch.allclose(kl_loss, kl_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XngGr18u7Nf"
      },
      "source": [
        "### **2.4 Module Initialization and Forwarding**\n",
        "\n",
        "Up to this point, we've worked with individual layers as standalone components. But real neural networks are built by combining these layers into larger structures. This is where `nn.Module` becomes essential.\n",
        "\n",
        "Every layer we've usedâ€”whether it's `nn.Linear`, `nn.Conv2d`, or `nn.ReLU`â€”is actually a subclass of `nn.Module`. This base class provides the infrastructure that makes PyTorch's modular design work.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IQIk_TRsfMK"
      },
      "source": [
        "#### **2.4.1 Custom Module**\n",
        "\n",
        "When building a custom network, you create a class that inherits from `nn.Module`. In `__init__`, you define your layers as attributes, storing them for later use. Then, you implement the `forward` method to specify exactly how data flows through these layers, calling them like functions.\n",
        "\n",
        "This design gives you three key benefits: **automatic parameter tracking** (all parameters from submodules are collected automatically), **seamless mode switching** (`.train()` and `.eval()` propagate through the entire module tree), and **clean composition** (modules can contain other modules, enabling hierarchical architectures like U-Nets or Transformers).\n",
        "\n",
        "As for the functional operations we just discussed, they're typically called inside the forward method, giving you precise control over the computation graph while the Module handles state management.\n",
        "\n",
        "Additionally, GPU movement is seamless: calling `.to(device)` on the parent module recursively transfers all parameters and buffers to the specified device. And please note that inputs must be moved manually at the same time, as PyTorch won't do this automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zEQjmZkmEa1",
        "outputId": "10236795-adfc-4a1a-823f-b4289482dd38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Output shape: torch.Size([4, 10])\n",
            "First conv weight device: cuda:0\n",
            "Training mode: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# A reusable block: demonstrates module composition\n",
        "class SimpleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # Store layers as attributes - parameters tracked automatically\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Call layers like functions, use functional ops for flexibility\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "        x = F.relu(x)  # Using functional activation\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# A larger network: composed of multiple blocks\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Modules can contain other modules - hierarchical architecture\n",
        "        self.block1 = SimpleBlock(3, 64)\n",
        "        self.block2 = SimpleBlock(64, 128)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Demonstration\n",
        "model = SimpleNet()\n",
        "\n",
        "# --- GPU MOVEMENT ---\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to GPU (all parameters/buffers moved automatically)\n",
        "model = model.to(device)\n",
        "# NOTE: .to() returns the module, so we reassign\n",
        "\n",
        "# Create input tensor\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "\n",
        "# CRITICAL: Must move input to same device as model!\n",
        "x = x.to(device)\n",
        "# NOTE: Inputs must be moved manually - PyTorch won't do this automatically\n",
        "\n",
        "# Forward pass now runs on GPU\n",
        "output = model(x)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Verify parameters are on GPU\n",
        "print(f\"First conv weight device: {model.block1.conv.weight.device}\")\n",
        "\n",
        "# Mode switching propagates through module tree\n",
        "model.train()\n",
        "print(f\"Training mode: {model.training}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weo-hVcVsoIS"
      },
      "source": [
        "#### **2.4.2 Containers**\n",
        "\n",
        "When building complex architectures, you'll need ways to organize layers and parameters beyond simple attributes. PyTorch provides several container classes designed specifically for this purpose.\n",
        "\n",
        "- `nn.Parameter` is how you tell PyTorch \"this tensor is a learnable parameter.\" When you assign a tensor as a Parameter in your module's `__init__`, it's automatically registered and will be updated by optimizers. Use this when creating custom layers with their own weights.\n",
        "\n",
        "- `nn.Embedding` serves as a specialized lookup table for handling discrete data, like words in NLP tasks or user IDs in recommendation systems. Instead of using one-hot encoding which is inefficient, it maps integer indices directly to dense vectors. Under the hood, it functions similarly to a large `nn.Parameter` matrix where each row represents an item, but it's optimized for retrieving and updating specific rows based on input indices.\n",
        "\n",
        "- `nn.Sequential` creates a simple stack of layers for straightforward architectures. When your network flows sequentially from input to output without branching, it keeps code clean by letting you pass layers directly during initialization.\n",
        "\n",
        "- For more flexible architectures, `nn.ModuleList` acts like a Python list that properly registers all submodules. This is essential when you need to iterate over layers, store collections of similar blocks, or build networks with variable depth. Unlike regular lists, ModuleList ensures parameter tracking works correctly. So please remember not to use a plain Python list in the `__init__` function to organize `nn.Module`, because it will not be registered as submodules.\n",
        "\n",
        "- Similarly, `nn.ModuleDict` provides dictionary-style access to modules using string keys. It's perfect for architectures with named components, conditional computation paths, or when loading configurations from files.\n",
        "\n",
        "These containers work seamlessly with custom modules, letting you build everything from simple feedforward networks to complex, multi-branch architectures while maintaining clean parameter management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8rVk3MkYSex",
        "outputId": "b3c721c0-58d4-43b1-e030-16ea116650e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification: torch.Size([2, 10])\n",
            "Auxiliary: torch.Size([2, 1])\n",
            "\n",
            "Scale: Parameter containing:\n",
            "tensor(1., requires_grad=True)\n",
            "Embedding parameters: torch.Size([1000, 64])\n",
            "Total parameters: 287244\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ContainerNet(nn.Module):\n",
        "    def __init__(self, vocab_size=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. nn.Parameter: register custom learnable tensors\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "        # 2. nn.Embedding: lookup table for discrete indices (Added here)\n",
        "        # Maps indices (0 to vocab_size-1) to vectors of dimension 64\n",
        "        self.embedding = nn.Embedding(vocab_size, 64)\n",
        "\n",
        "        # 3. nn.Sequential: stack layers linearly\n",
        "        # (Assuming input is now coming from embedding output for demonstration)\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 128, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        # 4. nn.ModuleList: list-like container that registers modules\n",
        "        self.blocks = nn.ModuleList([nn.Conv1d(128, 128, 3, padding=1) for _ in range(3)])\n",
        "\n",
        "        # 5. nn.ModuleDict: dict-like access to modules\n",
        "        self.heads = nn.ModuleDict({\n",
        "            'cls': nn.Linear(128, 10),\n",
        "            'aux': nn.Linear(128, 1)\n",
        "        })\n",
        "\n",
        "    def forward(self, x_indices, task='cls'):\n",
        "        # Step 1: Map integer indices to dense vectors\n",
        "        # Input shape: [Batch, Seq_Len] -> Output shape: [Batch, Seq_Len, 64]\n",
        "        x = self.embedding(x_indices)\n",
        "\n",
        "        # Permute for Conv1d (Batch, Channels, Seq_Len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = self.backbone(x) * self.scale\n",
        "\n",
        "        # Iterate over ModuleList\n",
        "        for block in self.blocks:\n",
        "            x = x + block(x)\n",
        "\n",
        "        x = F.adaptive_avg_pool1d(x, 1).flatten(1)\n",
        "\n",
        "        # Access via ModuleDict\n",
        "        return self.heads[task](x)\n",
        "\n",
        "# Demonstration\n",
        "model = ContainerNet()\n",
        "\n",
        "# Input is now discrete indices (e.g., words/IDs), not random floats\n",
        "x_indices = torch.randint(0, 1000, (2, 32))  # Batch size 2, Sequence length 32\n",
        "\n",
        "# Forward with different heads\n",
        "print(f\"Classification: {model(x_indices, task='cls').shape}\")\n",
        "print(f\"Auxiliary: {model(x_indices, task='aux').shape}\")\n",
        "\n",
        "# All parameters tracked automatically\n",
        "print(f\"\\nScale: {model.scale}\")\n",
        "# Check if embedding weights are registered\n",
        "print(f\"Embedding parameters: {model.embedding.weight.shape}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9FchVbMyMKk"
      },
      "source": [
        "### **2.5 Exercise: Simplified Vision Transformer Implementation**\n",
        "\n",
        "In this exercise, you will implement a simplified Vision Transformer (ViT) architecture. This task tests your ability to manipulate tensor dimensions (reshaping images into sequences), manage parameter registration with `nn.ModuleList`, and implement a specific forward pass logic without relying on pre-built transformer layers.\n",
        "\n",
        "**Scenario**:\n",
        "You are designing a model to process batches of images for a binary classification task. Instead of using Convolutional Neural Networks, you will treat the image pixels as a sequence of vectors and process them using a stack of Transformer blocks.\n",
        "\n",
        "**Challenge**:\n",
        "Implement two classes: `TransformerBlock` and `SimpleViT`. You must manually handle the reshaping of image tensors and the flow of data through the attention and feed-forward mechanisms.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "#### **1. Class: TransformerBlock**\n",
        "*   **Initialization**:\n",
        "    *   Accept `d_model` as an argument.\n",
        "    *   Initialize **Linear Layers** for Query, Key, and Value projections.\n",
        "    *   Initialize a single **Linear Layer** for the feed-forward part.\n",
        "    *   Initialize two **Layer Normalization** modules.\n",
        "*   **Forward Logic**:\n",
        "    Your forward pass must strictly follow this sequential data flow:\n",
        "    1.  **QKV Projection**: Input â†’ [Linear Layers] â†’ Q, K, V\n",
        "    2.  **Self-Attention**: Q, K, V â†’ [`F.scaled_dot_product_attention`] â†’ Attention Output\n",
        "    3.  **Residual & Norm 1**: (Input + Attention Output) â†’ [LayerNorm] â†’ Intermediate Latent\n",
        "    4.  **Feed-Forward**: Intermediate Latent â†’ [Linear Layer] â†’ [SiLU Activation] â†’ FFN Output\n",
        "    5.  **Residual & Norm 2**: (Intermediate Latent + FFN Output) â†’ [LayerNorm] â†’ Final Output\n",
        "\n",
        "#### **2. Class: SimpleViT**\n",
        "*   **Initialization**:\n",
        "    *   Accept `in_channels`, `d_model`, and `num_layers`.\n",
        "    *   Initialize an **Input Projection** linear layer (`in_channels` â†’ `d_model`).\n",
        "    *   Initialize a stack of blocks using **`nn.ModuleList`** containing `num_layers` instances of `TransformerBlock`.\n",
        "    *   Initialize a final **Output Head** linear layer (`d_model` â†’ 1) to produce a single logit.\n",
        "*   **Forward Logic**:\n",
        "    1.  **Flatten & Reshape**: Input Image (B, C, H, W) â†’ Sequence (B, H Ã— W, C).\n",
        "    2.  **Input Projection**: Sequence â†’ [Linear Layer] â†’ Embedded Sequence (now dimension is `d_model`).\n",
        "    3.  **Block Stack**: Pass the embedded sequence sequentially through every block in the `ModuleList`.\n",
        "    4.  **Global Pooling**: Average the output over the sequence dimension to get a single vector per batch item.\n",
        "    5.  **Output**: Pooled Vector â†’ [Output Head] â†’ Final Logit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDcdYzfdr3Wd",
        "outputId": "0e94f04b-078a-4cca-8452-5c6b635ceca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized on cuda. Input shape: torch.Size([2, 3, 32, 32])\n",
            "Forward pass successful. Output shape: torch.Size([2, 1])\n",
            "Backward pass successful. Gradients computed.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Initialize the parameterized layers for the block.\n",
        "        \"\"\"\n",
        "        # TODO: Initialize three Linear layers for Query, Key, and Value projections\n",
        "        # Hint: They should map from d_model to d_model\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # TODO: Initialize one Linear layer for the Feed-Forward network\n",
        "        self.w_ff = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # TODO: Initialize two LayerNorm modules\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Execute the forward pass logic.\n",
        "        Input x shape: [Batch, Seq_Len, d_model]\n",
        "        \"\"\"\n",
        "        # TODO: 1. QKV Projection: Pass x through w_q, w_k, and w_v\n",
        "        q = self.w_q(x)  # [B, S, D]\n",
        "        k = self.w_k(x)  # [B, S, D]\n",
        "        v = self.w_v(x)  # [B, S, D]\n",
        "\n",
        "        # TODO: 2. Self-Attention: Use F.scaled_dot_product_attention\n",
        "        attn_out = F.scaled_dot_product_attention(q, k, v)  # [B, S, D]\n",
        "\n",
        "        # TODO: 3. Residual & Norm 1: Add attention output to x, then apply ln1\n",
        "        x = x + attn_out  # [B, S, D]\n",
        "        x = self.ln1(x)  # [B, S, D]\n",
        "\n",
        "        # TODO: 4. Feed-Forward: Pass result through w_ff, then apply F.silu\n",
        "        ffn_out = self.w_ff(x)  # [B, S, D]\n",
        "        ffn_out = F.silu(ffn_out)  # [B, S, D]\n",
        "\n",
        "        # TODO: 5. Residual & Norm 2: Add FFN output to the input of step 4, then apply ln2\n",
        "        x = x + ffn_out  # [B, S, D]\n",
        "        x = self.ln2(x)  # [B, S, D]\n",
        "\n",
        "        return x # Replace with actual result\n",
        "\n",
        "\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, in_channels, d_model, num_layers):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Initialize the components of the Vision Transformer.\n",
        "        \"\"\"\n",
        "        # TODO: Initialize the Input Projection layer (Linear: in_channels -> d_model)\n",
        "        self.input_proj = nn.Linear(in_channels, d_model)\n",
        "\n",
        "        # TODO: Initialize a stack of TransformerBlocks using nn.ModuleList\n",
        "        # Hint: Create a list of 'num_layers' blocks and wrap it with nn.ModuleList()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # TODO: Initialize the Output Head (Linear: d_model -> 1)\n",
        "        self.head = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x shape: [Batch, Channels, Height, Width]\n",
        "        \"\"\"\n",
        "        # TODO: 1. Flatten & Reshape\n",
        "        # Transform x from [B, C, H, W] to [B, H*W, C]\n",
        "        # Hint: You might need .permute() before reshaping to keep channels last\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.reshape(B, C, -1)  # [B, C, HxW]\n",
        "        x = x.permute(0, 2, 1)  # [B, HxW, C]\n",
        "\n",
        "        # TODO: 2. Input Projection\n",
        "        # Project the sequence to [B, H*W, d_model]\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # TODO: 3. Block Stack\n",
        "        # Iterate through self.blocks and pass the sequence through each one\n",
        "        for block in self.blocks:\n",
        "            x = block(x)  # [B, HxW, d_model]\n",
        "\n",
        "        # TODO: 4. Global Pooling\n",
        "        # Average the output over the sequence dimension (dim=1) to get [B, d_model]\n",
        "        x = x.permute(0, 2, 1)  # [B, d_model, HxW]\n",
        "        ###### 2D implement\n",
        "        # x = x.reshape(B, -1, H, W)  # [B, d_model, H, W]\n",
        "        # x = F.adaptive_avg_pool2d(x, output_size=(1, 1))  # [B, d_model, 1, 1]\n",
        "        ###### 1D implement\n",
        "        x = F.adaptive_avg_pool1d(x, output_size=(1,))  # [B, d_model, 1]\n",
        "        x = x.flatten(1)  # [B, d_model]\n",
        "\n",
        "        # TODO: 5. Output\n",
        "        # Project to the final logit [B, 1]\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x # Replace with actual result\n",
        "\n",
        "# ==========================================\n",
        "# Verification Code\n",
        "# ==========================================\n",
        "# 1. Setup\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "B, C, H, W = 2, 3, 32, 32\n",
        "d_model = 64\n",
        "num_layers = 2\n",
        "\n",
        "# 2. Initialize Model and Input\n",
        "model = SimpleViT(in_channels=C, d_model=d_model, num_layers=num_layers).to(device)\n",
        "x = torch.randn(B, C, H, W).to(device)\n",
        "\n",
        "print(f\"Model initialized on {device}. Input shape: {x.shape}\")\n",
        "\n",
        "# 3. Forward Pass\n",
        "try:\n",
        "    y = model(x)\n",
        "    print(f\"Forward pass successful. Output shape: {y.shape}\")\n",
        "    assert y.shape == (B, 1), f\"Expected output shape (B, 1), got {y.shape}\"\n",
        "except Exception as e:\n",
        "    print(f\"Forward pass failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 4. Backward Pass\n",
        "try:\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "    print(\"Backward pass successful. Gradients computed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Backward pass failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqdXOc7cZZgt"
      },
      "source": [
        "## Part 3: Training Pipeline\n",
        "\n",
        "*   **Data Preparation**: Learn how to wrap your data into a `Dataset` and use `DataLoader` to send batches to the GPU efficiently.\n",
        "*   **Optimization**: Learn how to update model parameters using Loss Functions, Optimizers, and Learning Rate Schedulers.\n",
        "*   **Saving & Loading**: Learn how to save your trained model weights and resume training from where you left off.\n",
        "*   **Training Loop**: Learn how to put everything togetherâ€”data, model, and optimizerâ€”into a complete loop to train and validate your network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFQOx6hRGwSV"
      },
      "source": [
        "### **3.1 Dataset & Dataloader**\n",
        "\n",
        "In this section, we will learn how to feed data into our model. In the real world, data comes in messy formatsâ€”images in folders, CSV files, text logs, or raw binary files. PyTorch provides a standard two-step pipeline to handle this: the `torch.utils.data.Dataset` wraps your raw data, and the `torch.utils.data.DataLoader` handles the heavy lifting of batching and parallel loading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGdmZ9dEazYn"
      },
      "source": [
        "#### **3.1.1 Dataset**\n",
        "\n",
        "First, let's talk about the **Dataset**. Think of this as an adapter. No matter how weird your original data format is, your goal is to wrap it into a standard Python class so PyTorch can understand it.\n",
        "\n",
        "To create a custom dataset, you inherit from `torch.utils.data.Dataset` and implement three specific methods. It's crucial to understand **where** to put your codeâ€”in `__init__` or `__getitem__`â€”because this affects performance.\n",
        "\n",
        "1.  **`__init__` (The Setup)**:\n",
        "    *   This runs only **once** when you create the dataset object.\n",
        "    *   **What to do here**: Load file paths, parse CSV headers, or build a dictionary mapping indices to file locations.\n",
        "    *   **Key Rule**: Do *lightweight* organization here. Do **not** load all your images or heavy files into memory here unless the dataset is tiny. You just want to create a map or a list so the dataset knows \"what is where.\"\n",
        "\n",
        "2.  **`__len__` (The Size)**:\n",
        "    *   This simply returns the total number of samples. PyTorch needs this to know how many iterations make up an \"epoch.\"\n",
        "\n",
        "3.  **`__getitem__` (The Fetcher)**:\n",
        "    *   This is where the real work happens. It runs every time the training loop asks for a specific sample (e.g., \"Give me the 5th image\").\n",
        "    *   **What to do here**: Load the actual image from disk, convert text to tokens, apply data augmentation, or normalize pixel values.\n",
        "    *   **Why here?**: We put the heavy, time-consuming operations (like reading files or resizing images) here because this method is called \"on demand.\" This lazy loading strategy saves memory and allows us to use parallel workers later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5G8cJcpfBB0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. The Dummy Data Source\n",
        "# ==========================================\n",
        "# Simulate a raw data dump (e.g., from a JSON file or DB query)\n",
        "# It contains paths, metadata, and some \"bad\" data we need to filter.\n",
        "dummy_raw_data = []\n",
        "for i in range(100):\n",
        "    dummy_raw_data.append({\n",
        "        \"file_id\": f\"img_{i:03d}\",\n",
        "        \"path\": f\"/data/images/img_{i}.png\",\n",
        "        \"category\": random.randint(0, 9),\n",
        "        \"is_corrupted\": (i % 10 == 0) # Mark every 10th item as \"corrupted\"\n",
        "    })\n",
        "\n",
        "# ==========================================\n",
        "# 2. The Dataset Wrapper\n",
        "# ==========================================\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, raw_data):\n",
        "        \"\"\"\n",
        "        1. __init__ (The Setup):\n",
        "        We receive the raw data list. Our job is to filter and organize it.\n",
        "        We build a clean 'index' here but do NOT load heavy tensors yet.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.clean_samples = []\n",
        "\n",
        "        # Filter out corrupted data during initialization\n",
        "        for item in raw_data:\n",
        "            if not item[\"is_corrupted\"]:\n",
        "                self.clean_samples.append(item)\n",
        "\n",
        "        print(f\"[Init] Raw data: {len(raw_data)} -> Valid samples: {len(self.clean_samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        2. __len__ (The Size):\n",
        "        Returns the count of valid samples.\n",
        "        \"\"\"\n",
        "        return len(self.clean_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        3. __getitem__ (The Fetcher):\n",
        "        Runs on demand. This is where we simulate heavy loading.\n",
        "        \"\"\"\n",
        "        # Step A: Retrieve metadata using the clean index\n",
        "        item_info = self.clean_samples[idx]\n",
        "\n",
        "        # Step B: Simulate heavy IO (loading image from 'path')\n",
        "        # We use the ID to seed randomness so the same index always yields the same 'image'\n",
        "        seed = int(item_info[\"file_id\"].split('_')[1])\n",
        "        torch.manual_seed(seed)\n",
        "        image_tensor = torch.randn(3, 64, 64) # Simulated image\n",
        "\n",
        "        # Step C: Get the label\n",
        "        label = torch.tensor(item_info[\"category\"], dtype=torch.long)\n",
        "\n",
        "        return image_tensor, label\n",
        "\n",
        "# Wrap the dummy data\n",
        "dataset = CustomDataset(dummy_raw_data)\n",
        "\n",
        "# Test fetching one item\n",
        "img, target = dataset[0]\n",
        "print(f\"Sample 0 - Shape: {img.shape}, Label: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--HybAUEbwSN"
      },
      "source": [
        "#### **3.1.2 The DataLoader**\n",
        "\n",
        "Now that we have a Dataset, we need a **DataLoader**. If the Dataset is a map, the DataLoader is the delivery truck. It grabs items from your Dataset, groups them into batches, and delivers them to your training loop.\n",
        "\n",
        "Why do we need it? Because training on one sample at a time is slow. We want to process data in batches, and we want to load the next batch while the GPU is busy processing the current one.\n",
        "\n",
        "Here is how the DataLoader works and the key parameters you need to know:\n",
        "\n",
        "1.  **Workers (`num_workers`)**:\n",
        "    *   The DataLoader can spawn multiple sub-processes (workers) to call your `__getitem__` method in parallel.\n",
        "    *   If `num_workers=0`, the main process does everything (slow).\n",
        "    *   If `num_workers=4`, four separate processes are reading files and processing data simultaneously. This is why we put the heavy work in `__getitem__`â€”so these workers can share the load!\n",
        "\n",
        "2.  **Batch Size (`batch_size`)**:\n",
        "    *   This determines how many samples are stacked together into a single tensor before being sent to the model. Larger batches can speed up training but require more GPU memory.\n",
        "\n",
        "3.  **Memory Pinning (`pin_memory=True`)**:\n",
        "    *   This is a common point of confusion. Setting this to `True` does **not** automatically move data to the GPU.\n",
        "    *   Instead, it allocates the data in a special type of CPU memory (page-locked memory) that is faster to copy to the GPU.\n",
        "    *   **The Workflow**: The workers load data to CPU RAM â†’ `pin_memory` locks it in a special CPU buffer â†’ You manually call `.to('cuda')` in your loop, which happens much faster because the memory is pinned.\n",
        "\n",
        "4.  **Prefetch Factor**:\n",
        "    *   This controls how many batches each worker loads in advance. It ensures that when the GPU finishes a batch, the next one is already waiting at the door, minimizing the time the GPU sits idle.\n",
        "\n",
        "5. **Shuffling (`shuffle=True`)**:\n",
        "    *   This is critical for training. It randomizes the order of samples every epoch.\n",
        "    *   **Why?** If you feed the model data in the same order every time (e.g., all \"Cats\" then all \"Dogs\"), the model might learn patterns based on the *order* rather than the content, leading to poor generalization.\n",
        "    *   *Note: For validation or testing, we usually set `shuffle=False` because the order doesn't affect the metric calculation, and it helps to compare results consistently.*\n",
        "\n",
        "6. **Dropping the Last Batch (`drop_last=True`)**:\n",
        "    *   Often, your dataset size isn't perfectly divisible by the `batch_size`. This leaves a \"leftover\" batch at the end that is smaller than the rest.\n",
        "    *   **Why drop it?** Some layers (like Batch Normalization) behave unstably or crash if they receive a batch with only 1 sample. Also, having a variable batch size can mess up metric calculations or hardware optimization. Setting `drop_last=True` simply discards those few remaining samples to ensure every batch has the exact same shape.\n",
        "\n",
        "In summary: The **Dataset** defines *how* to read a single item, and the **DataLoader** organizes *how* to grab many items efficiently using parallel workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve3YZ2iLgMsO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assume 'dataset' is the instance created in 3.1.1\n",
        "# dataset = CustomDataset(dummy_raw_data)\n",
        "\n",
        "# ==========================================\n",
        "# 3.1.2 The DataLoader\n",
        "# ==========================================\n",
        "\n",
        "# Initialize the DataLoader\n",
        "# This orchestrates the workers to fetch from 'dataset' in parallel\n",
        "train_loader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=8,        # Group 8 samples into one batch\n",
        "    shuffle=True,        # Randomize order every epoch\n",
        "    num_workers=2,       # Use 2 parallel subprocesses\n",
        "    drop_last=True,      # Drop the last batch if size < 8\n",
        "    pin_memory=True      # Speed up CPU -> GPU transfer\n",
        ")\n",
        "\n",
        "print(f\"DataLoader ready. Total batches: {len(train_loader)}\\n\")\n",
        "\n",
        "# Simulate the Training Loop\n",
        "print(\"--- Starting Epoch 1 (Shuffled) ---\")\n",
        "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    # inputs: [8, 3, 64, 64]\n",
        "    # targets: [8]\n",
        "\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(f\"  Labels: {targets.tolist()}\") # Observe the random order\n",
        "    print(f\"  Input Batch Shape: {inputs.shape}\")\n",
        "\n",
        "    # Stop after 2 batches for demonstration\n",
        "    if batch_idx == 1:\n",
        "        break\n",
        "\n",
        "print(\"\\n--- Starting Epoch 2 (Order changes!) ---\")\n",
        "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(f\"  Labels: {targets.tolist()}\")\n",
        "    if batch_idx == 0: break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmnxIdyGG2sf"
      },
      "source": [
        "### **3.2 Optimizer & LRScheduler**\n",
        "\n",
        "Now that we have our data ready, we need to discuss how the model actually learns. This is the job of the **Optimizer**.\n",
        "\n",
        "In this section, we will first dive into the mathematics of the three most common optimizers used in modern deep learning: **SGD**, **Adam**, and **AdamW** (with decoupled weight decay) . We will look at their update formulas to understand exactly how they calculate gradients.\n",
        "\n",
        "However, as training progresses, the model's need for a learning rate changesâ€”usually starting high to learn quickly and decaying later to fine-tune. While PyTorch provides various `LRScheduler` classes to handle this, they can often be stateful and tricky to debug. Instead, I will teach you a more flexible, stateless approach manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl3qyTEqpmsF"
      },
      "source": [
        "#### **3.2.1 SGD (Stochastic Gradient Descent)**\n",
        "\n",
        "Let's start with the cornerstone of all optimizers: **SGD**. It's the most basic and fundamental algorithm. Think of it as taking a small step directly opposite to the gradient. The gradient points \"uphill\" (where the loss is highest), so we step \"downhill.\"\n",
        "\n",
        "The simplest form of the update rule looks like this:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\cdot g_t\n",
        "$$\n",
        "\n",
        "Here, $\\theta_{t+1}$ are the new parameters, $\\theta_t$ are the current ones, $\\eta$ is the learning rate, and $g_t$ is the gradient we just calculated through backward.\n",
        "\n",
        "When you create an SGD optimizer in PyTorch, like `torch.optim.SGD(model.parameters(), ...)`, you'll mainly deal with these four arguments:\n",
        "\n",
        "1.  **`lr` (Learning Rate, $\\eta$)**:\n",
        "    *   This is the most important hyperparameter. It's the $\\eta$ in our formula.\n",
        "    *   **What it does**: It controls the size of the step you take.\n",
        "    *   **Analogy**: If you're walking down a hill, `lr` is your step size. Too large, and you might overshoot the bottom and bounce around. Too small, and it will take forever to get there.\n",
        "\n",
        "2.  **`weight_decay` (L2 Regularization)**:\n",
        "    *   This is a technique to prevent overfitting by penalizing large parameter values.\n",
        "    *   **What it does**: It adds a term to the gradient that pushes the weights closer to zero. It modifies the gradient `g_t` *before* it's used in the update rule. The effective gradient becomes\n",
        "    $g_t + \\lambda \\cdot \\theta_t$, where $\\lambda$ is the `weight_decay` value.\n",
        "    *   **Key Rule**: It helps keep your model's weights small and simple, which often leads to better generalization.\n",
        "\n",
        "3.  **`momentum`**:\n",
        "    *   Vanilla SGD can be slow or stuck, especially in long, narrow valleys of the loss landscape. Momentum helps smooth out the journey.\n",
        "    *   **What it does**: It introduces a **velocity** term, let's call it $v_t$, which is a moving average of past gradients. Instead of just using the current gradient, we use this velocity to update the parameters.\n",
        "    *   **The new formula**:\n",
        "      \\begin{aligned}\n",
        "      & v_{t+1} = \\beta \\cdot v_t + g_t \\\\\n",
        "      & \\theta_{t+1} = \\theta_t - \\eta \\cdot v_{t+1}\n",
        "      \\end{aligned}\n",
        "\n",
        "    *   Here, $\\beta$ is the `momentum` coefficient (e.g., 0.9). This update is like a heavy ball rolling downhillâ€”it accumulates momentum from past gradients, helping it power through small bumps and stay on a more stable path.\n",
        "\n",
        "4.  **`nesterov` (Nesterov Accelerated Gradient)**:\n",
        "    *   This is a slight but powerful modification to standard momentum.\n",
        "    *   **What it does**: Instead of combining the current gradient with the previous velocity, Nesterov's method adjusts the gradient itself using the velocity. It calculates the gradient not at the current position $\\theta_t$, but at a position slightly ahead in the direction of the momentum.\n",
        "    *   While the \"looking ahead\" intuition is common, a more direct, equivalent formula is easier to implement and understand. It modifies how the velocity term is constructed:\n",
        "        \\begin{aligned}\n",
        "        & v_{t+1} = \\beta \\cdot v_t + g_t \\\\\n",
        "        & \\theta_{t+1} = \\theta_t - \\eta \\cdot (g_t + \\beta \\cdot v_{t+1})\n",
        "        \\end{aligned}\n",
        "    *   Notice the difference: the update to $\\theta$ now includes both the current gradient $g_t$ and the *new* velocity term $v_{t+1}$ (scaled by $\\beta$). This gives the momentum a \"correction factor,\" often leading to faster convergence. When you set `nesterov=True`, PyTorch uses this improved update rule.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551hgHW3048x"
      },
      "source": [
        "#### **3.2.2 Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "Next up is **Adam**. If SGD is the reliable manual transmission car, Adam is the modern automatic. It's often the default, go-to optimizer for many deep learning tasks because it combines the best of two worlds: the momentum idea from SGD and an adaptive learning rate mechanism.\n",
        "\n",
        "The big idea behind Adam is to maintain **two** separate moving averages for each parameter, not just one.\n",
        "\n",
        "1.  **The First Moment (The Momentum):**\n",
        "    *   This is almost identical to the momentum we saw in SGD. It's an moving average of the gradients. Let's call it $m_t$.\n",
        "    *   **What it does**: It tracks the *direction* of the updates, helping to smooth out the path and accelerate in a consistent direction. It's the \"velocity\" of our optimization.\n",
        "    *   The formula is:\n",
        "        $$\n",
        "        m_{t+1} = \\beta_1 \\cdot m_t + (1 - \\beta_1) \\cdot g_t\n",
        "        $$\n",
        "    *   Here, $\\beta_1$ is a hyperparameter, typically around 0.9. It controls how much we rely on past gradients versus the current one.\n",
        "\n",
        "2.  **The Second Moment (The Adaptive Part):**\n",
        "    *   This is the new, clever part of Adam. It's a moving average of the *squared* gradients. Let's call it $v_t$.\n",
        "    *   **What it does**: It tracks the *variability* or \"spread\" of the gradients for each parameter. Think of it as a measure of confidence. If a gradient is consistently large or jumps around a lot, $v_t$ will be large. If the gradient is small and stable, $v_t$ will be small.\n",
        "    *   The formula is:\n",
        "        $$\n",
        "        v_{t+1} = \\beta_2 \\cdot v_t + (1 - \\beta_2) \\cdot g_t^2\n",
        "        $$\n",
        "    *   $\\beta_2$ is another hyperparameter, usually around 0.999.\n",
        "\n",
        "**Putting It All Together**\n",
        "\n",
        "Adam uses these two moments to scale the learning rate for *each parameter individually*. The final update rule looks like this:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Let's break that down:\n",
        "*   The $\\hat{m}_{t+1}$ and $\\hat{v}_{t+1}$ are bias-corrected versions of the moments. Because our moving averages, $m_t$ and $v_t$, start at zero, they are biased towards zero during the first few training steps. To counteract this, Adam scales them up using these formulas:\n",
        "$$\n",
        "\\hat{m}_{t+1} = \\frac{m_{t+1}}{1 - \\beta_1^{t+1}} \\quad \\text{and} \\quad \\hat{v}_{t+1} = \\frac{v_{t+1}}{1 - \\beta_2^{t+1}}\n",
        "$$\n",
        "The factors, $1 - \\beta_1^{t+1}$ and $1 - \\beta_2^{t+1}$, start small and approach 1 as the step count $t$ increases, effectively making this correction vanish over time.\n",
        "*   The key part is the factor: $\\sqrt{\\hat{v}_{t+1}}$. We are dividing the momentum-based update by the square root of the second moment.\n",
        "    *   If a parameter's gradients have been all over the place (large $v_t$), the effective learning rate for it gets smaller. The optimizer becomes more cautious.\n",
        "    *   If a parameter's gradients are small and stable (small $v_t$), the effective learning rate gets larger, allowing for faster progress.\n",
        "*   $\\epsilon$ (epsilon) is just a tiny number (like `1e-8`) added for numerical stability to prevent division by zero.\n",
        "\n",
        "---\n",
        "\n",
        "In `torch.optim.Adam`, the main arguments you'll tune are:\n",
        "*   `lr`: The initial, global learning rate ($\\eta$).\n",
        "*   `betas`: A tuple `(beta1, beta2)` containing the decay rates for the first and second moments. The defaults `(0.9, 0.999)` are a great starting point.\n",
        "*   `weight_decay`: Just like in SGD, this applies L2 regularization. However, how Adam implements this is a bit tricky and leads us directly to our next optimizer, AdamW. We'll discuss the specifics there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l832Lx4s3SaF"
      },
      "source": [
        "#### **3.2.3 AdamW (Adam with Decoupled Weight Decay)**\n",
        "\n",
        "Finally, let's look at **AdamW**. This is a simple but important fix for Adam, and it's all about how `weight_decay` is handled.\n",
        "\n",
        "**The Problem in Adam**\n",
        "\n",
        "In the original Adam optimizer, `weight_decay` is implemented as L2 regularization. This means it adds the weight decay term directly to the gradient *before* calculating the moving averages:\n",
        "\n",
        "$$\n",
        "g_t \\leftarrow g_t + \\lambda \\cdot \\theta_t\n",
        "$$\n",
        "\n",
        "The issue is that this \"pollutes\" the second moment estimate ($v_t$). The adaptive learning rate now depends on the size of your weights, not just the history of your gradients, which can lead to suboptimal training.\n",
        "\n",
        "**The AdamW Solution: Decoupling**\n",
        "\n",
        "AdamW \"decouples\" the weight decay from the gradient update. It uses the original, \"clean\" gradient to compute the Adam step and then applies the weight decay directly to the weights at the end.\n",
        "\n",
        "The update rule for AdamW is:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\underbrace{\\eta \\cdot \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon}}_{\\text{Adam Update Step}} - \\underbrace{\\eta \\cdot \\lambda \\cdot \\theta_t}_{\\text{Weight Decay Step}}\n",
        "$$\n",
        "\n",
        "See the difference? The weight decay ($\\lambda \\cdot \\theta_t$) is now a separate term subtracted at the end, scaled only by the learning rate $\\eta$. It no longer interferes with the adaptive moment calculations.\n",
        "\n",
        "This small change makes the effect of `weight_decay` much more predictable and effective.\n",
        "\n",
        "**Rule of Thumb**: If you plan to use weight decay with an Adam-style optimizer, you should almost always use **AdamW**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os1RDXfD4bxD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Setup: A simple model and some dummy data\n",
        "# =============================================================================\n",
        "input_data = torch.randn(1, 10)\n",
        "target = torch.randn(1, 1)\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# We create three identical copies of the model.\n",
        "# This is crucial so that the initial weights and gradients are the same for everyone.\n",
        "model_sgd = copy.deepcopy(model)\n",
        "model_adam = copy.deepcopy(model)\n",
        "model_adamw = copy.deepcopy(model)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Hyperparameters (Try changing these values and see what happens!)\n",
        "# =============================================================================\n",
        "lr = 0.001\n",
        "weight_decay = 0.01\n",
        "\n",
        "# --- For SGD ---\n",
        "momentum = 0.9\n",
        "nesterov = False  # Try setting this to True\n",
        "\n",
        "# --- For Adam & AdamW ---\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Define the optimizers with their respective parameters\n",
        "# =============================================================================\n",
        "opt_sgd = torch.optim.SGD(\n",
        "    model_sgd.parameters(),\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    nesterov=nesterov\n",
        ")\n",
        "\n",
        "opt_adam = torch.optim.Adam(\n",
        "    model_adam.parameters(),\n",
        "    lr=lr,\n",
        "    betas=betas,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "opt_adamw = torch.optim.AdamW(\n",
        "    model_adamw.parameters(),\n",
        "    lr=lr,\n",
        "    betas=betas,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "optimizers = {\n",
        "    \"SGD\": (model_sgd, opt_sgd),\n",
        "    \"Adam\": (model_adam, opt_adam),\n",
        "    \"AdamW\": (model_adamw, opt_adamw),\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Perform a single update step and record the changes\n",
        "# =============================================================================\n",
        "updated_weights = {}\n",
        "\n",
        "print(\"Initial model weights (first 5 values):\")\n",
        "print(model.weight.data.flatten()[:5].numpy())\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for name, (m, opt) in optimizers.items():\n",
        "    # Store the weights before the update\n",
        "    initial_weights = m.weight.clone().detach()\n",
        "\n",
        "    # Forward and backward pass to compute gradients\n",
        "    # This part is identical for all three models\n",
        "    opt.zero_grad()\n",
        "    prediction = m(input_data)\n",
        "    loss = nn.MSELoss()(prediction, target)\n",
        "    loss.backward()\n",
        "\n",
        "    # The optimizer applies its unique update rule\n",
        "    opt.step()\n",
        "\n",
        "    # Get the weights AFTER the update\n",
        "    final_weights = m.weight.clone().detach()\n",
        "\n",
        "    # The difference shows exactly what update was applied\n",
        "    updated_weights[name] = final_weights - initial_weights\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Compare the results\n",
        "# =============================================================================\n",
        "print(\"Change in weights after one step (first 5 values):\")\n",
        "for name, update in updated_weights.items():\n",
        "    print(f\"{name:>6}: {update.flatten()[:5].numpy()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qg3DwOo5hmy"
      },
      "source": [
        "#### **3.2.4 Manually Adjust Learning Rate**\n",
        "\n",
        "Alright, we've covered the optimizers. Now, let's discuss how to control the learning rate over time. While PyTorch offers `LRScheduler` classes, I'll show you a simpler, more transparent way to do it manually, which is easier to debug and gives you more control.\n",
        "\n",
        "The key is to directly interact with the optimizer's configuration.\n",
        "\n",
        "*   **The `optimizer.param_groups` Attribute:** An optimizer stores its settings in an attribute called `param_groups`. This is a list of dictionaries, where each dictionary holds the configuration for a specific group of model parameters, such as the learning rate (`lr`), weight decay, and momentum. Most of the time, this list contains just one dictionary for all your model's parameters.\n",
        "\n",
        "*   **How the Optimizer Uses the Learning Rate:** Every time you call `optimizer.step()`, it internally accesses the `'lr'` value from each dictionary in `param_groups` to perform the parameter updates for that group.\n",
        "\n",
        "*   **Manually Adjusting the Learning Rate:** Since the optimizer reads this value on every step, we can simply modify it ourselves right before the update. The logic is straightforward: loop through the `param_groups` list and set a new value for the `'lr'` key.\n",
        "\n",
        "    ```python\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_learning_rate\n",
        "    ```\n",
        "\n",
        "*   **The Advantage:** This approach is **stateless**. You don't need a separate scheduler object to track epochs or steps. You simply calculate the learning rate you need for the current iteration and set it directly. This method is explicit, flexible, and gives you complete freedom to implement any custom learning rate schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bzed1c-7M_0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def adjust_learning_rate(optimizer, initial_lr, step, decay_factor):\n",
        "    \"\"\"Sets the learning rate for a given step based on an initial LR and a decay factor.\"\"\"\n",
        "    new_lr = initial_lr * (decay_factor ** step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "    return new_lr\n",
        "\n",
        "# 1. A simple model, optimizer, and loss function\n",
        "model = nn.Linear(5, 1)\n",
        "initial_lr = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# 2. Dummy data\n",
        "dummy_input = torch.randn(1, 5)\n",
        "dummy_target = torch.randn(1, 1)\n",
        "\n",
        "print(f\"Initial bias value: {model.bias.item():.4f}\\n\")\n",
        "\n",
        "# 3. A simple training loop for 3 steps\n",
        "for step in range(3):\n",
        "    print(f\"--- Step {step} ---\")\n",
        "\n",
        "    # 4. Set the learning rate for the CURRENT step\n",
        "    current_lr = adjust_learning_rate(optimizer, initial_lr, step, decay_factor=0.5)\n",
        "    print(f\"LR for this step is set to: {current_lr:.4f}\")\n",
        "\n",
        "    # Standard training procedure\n",
        "    optimizer.zero_grad()\n",
        "    output = model(dummy_input)\n",
        "    loss = loss_fn(output, dummy_target)\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer updates parameters using the LR we just set\n",
        "    bias_before_update = model.bias.item()\n",
        "    optimizer.step()\n",
        "    bias_after_update = model.bias.item()\n",
        "\n",
        "    print(f\"Bias changed from {bias_before_update:.4f} to {bias_after_update:.4f}\")\n",
        "    print(f\"Change amount: {(bias_after_update - bias_before_update):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3iYGIDIlXpq"
      },
      "source": [
        "### **3.3 Model Persistence & Training Loop**\n",
        "\n",
        "Alright, we've covered datasets, dataloaders, and optimizers. Now it's time to put everything together. This section is about two key things: how to save and load your model, and the structure of a standard training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6AhWZ_fOiVo"
      },
      "source": [
        "#### **3.3.1 Model Persistence**\n",
        "\n",
        "Training a generative model can take hours or even days. You definitely don't want to lose your progress if your program crashes or you need to pause. This is where model persistence comes in.\n",
        "\n",
        "The standard way to save things in PyTorch is with `torch.save`. But **what** exactly should you save?\n",
        "\n",
        "1.  **Saving the Model's State (`state_dict`)**\n",
        "    *   The best practice is **not** to save the entire model object. Instead, save its **`state_dict`**.\n",
        "    *   **What is a `state_dict`?** It's a simple Python dictionary that maps each layer to its learnable parameters (weights and biases). It's clean, lightweight, and portable.\n",
        "    *   **How to save:** `torch.save(model.state_dict(), 'model.pth')`\n",
        "\n",
        "2.  **Saving the Optimizer's State**\n",
        "    *   If you plan to **resume training**, you must also save the optimizer's state.\n",
        "    *   **Why?** The optimizer's `state_dict` contains its internal buffers and parameters, like the momentum values in SGD or the running averages in Adam. Without these, you're not truly resuming; you're starting the optimization process from scratch.\n",
        "    *   **How to save:** `torch.save(optimizer.state_dict(), 'optimizer.pth')`\n",
        "\n",
        "3.  **Saving the Full State for Reproducibility**\n",
        "    *   For most simple experiments, saving the model and optimizer is enough. For quick tests, you might even get away with just saving the model.\n",
        "    *   However, when dealing with large-scale models or when strict **reproducibility** is required, you need to control every source of randomness. This means saving more than just weights.\n",
        "    *   In advanced scenarios, you also save the state of:\n",
        "        *   **The DataLoader**: This can be surprisingly detailed. It includes not just the random seed for shuffling, but potentially even which worker is processing which batch.\n",
        "        *   **Random Number Generators (RNGs)**: You need to save the state of every RNG you use. The common ones are `torch` (for CPU), `torch.cuda` (for GPU), Python's built-in `random`, and `numpy.random`.\n",
        "    *   Why go through all this trouble? These details, while seeming minor, are critical for ensuring that if you stop and restart a training run, it continues from the *exact* same state. In large-scale distributed training, for example, we might generate a specific random number for each GPU at each step, ensuring every random operation in the training loop is traceable and repeatable.\n",
        "    *   We won't require you to implement this level of detail in this tutorial, but it's crucial to know that saving just the model and optimizer is often not enough to perfectly preserve and resume a training state.\n",
        "\n",
        "**Loading the State**\n",
        "\n",
        "So, we've covered saving. How do we load the state back? The process is straightforward but has one critical rule: you must first create an instance of your model and optimizer *before* you can load their states. The saved `state_dict` only contains the parameters, not the model's architecture itself.\n",
        "\n",
        "Once you have your empty model and optimizer objects, you use `torch.load()` to read the checkpoint file from disk. Then, you call the `.load_state_dict()` method on your model and optimizer to populate them with the saved weights and internal states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ME2Hx98VejJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Setup: Create a simple model, optimizer, and some dummy data\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Linear(10, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "inputs = torch.randn(4, 10)\n",
        "targets = torch.randn(4, 1)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "model_original = SimpleNet()\n",
        "optimizer_original = optim.SGD(model_original.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 2. Simulate a few training steps\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- Running a few training steps... ---\")\n",
        "for _ in range(3):\n",
        "    optimizer_original.zero_grad()\n",
        "    outputs = model_original(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer_original.step()\n",
        "    print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 3. Capture the state of the original model and optimizer\n",
        "# ----------------------------------------------------------------------------\n",
        "loss_before_save = criterion(model_original(inputs), targets)\n",
        "\n",
        "# Get the first layer's weight parameter from the ORIGINAL model\n",
        "param_original = model_original.seq[0].weight\n",
        "# Inspect the momentum buffer for this specific parameter\n",
        "momentum_before_save = optimizer_original.state[param_original]['momentum_buffer']\n",
        "\n",
        "print(f\"\\n[Original] Loss before saving: {loss_before_save.item():.4f}\")\n",
        "print(f\"[Original] Optimizer momentum for first layer (first 2 values):\\n{momentum_before_save[0, :2]}\\n\")\n",
        "\n",
        "# 4. Save the state_dicts\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- Saving model and optimizer state_dicts... ---\")\n",
        "torch.save(model_original.state_dict(), 'model_checkpoint.pth')\n",
        "torch.save(optimizer_original.state_dict(), 'optimizer_checkpoint.pth')\n",
        "\n",
        "# 5. Create NEW instances and load the saved states\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"--- Creating new model and optimizer and loading states... ---\")\n",
        "\n",
        "model_loaded = SimpleNet()\n",
        "optimizer_loaded = optim.SGD(model_loaded.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "model_loaded.load_state_dict(torch.load('model_checkpoint.pth'))\n",
        "optimizer_loaded.load_state_dict(torch.load('optimizer_checkpoint.pth'))\n",
        "\n",
        "# 6. Verify that the states are identical\n",
        "# ----------------------------------------------------------------------------\n",
        "model_original.eval()\n",
        "model_loaded.eval()\n",
        "\n",
        "loss_after_load = criterion(model_loaded(inputs), targets)\n",
        "print(f\"\\n[Loaded] Loss after loading: {loss_after_load.item():.4f}\")\n",
        "\n",
        "# To inspect the loaded optimizer's state, we must use the corresponding parameter\n",
        "# from the NEW model (`model_loaded`) as the key.\n",
        "param_loaded = model_loaded.seq[0].weight\n",
        "momentum_after_load = optimizer_loaded.state[param_loaded]['momentum_buffer']\n",
        "print(f\"[Loaded] Optimizer momentum for first layer (first 2 values):\\n{momentum_after_load[0, :2]}\\n\")\n",
        "\n",
        "# Final check\n",
        "assert torch.allclose(loss_before_save, loss_after_load), \"Model states do not match!\"\n",
        "assert torch.allclose(momentum_before_save, momentum_after_load), \"Optimizer states do not match!\"\n",
        "print(\"âœ… Success! The loaded model and optimizer have the exact same state as the originals.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ESeOa4RE9Z"
      },
      "source": [
        "#### **3.3.2 The Training Loop**\n",
        "\n",
        "Now, let's assemble the complete training loop. This is the core engine of your deep learning project, and understanding its structure is fundamental. A typical training iteration consists of a sequence of well-defined steps.\n",
        "\n",
        "1.  **Fetch a Batch of Data**:\n",
        "    First, we get a batch of data from our `DataLoader`. A crucial but easily forgotten step is to move this data to the correct device, which is typically your GPU.\n",
        "    *   `inputs, targets = data`\n",
        "    *   `inputs, targets = inputs.to(device), targets.to(device)`\n",
        "\n",
        "2.  **Adjust Learning Rate (Optional)**:\n",
        "    Next, as we discussed in section 3.2.4, you might need to manually adjust the learning rate. This is a good place to do itâ€”right at the start of an iteration. This ensures the optimizer will use the updated learning rate for the current step. For example, you might implement a learning rate warmup or a decay schedule based on the current iteration or epoch number.\n",
        "    *   `for param_group in optimizer.param_groups: param_group['lr'] = new_lr`\n",
        "\n",
        "3.  **Perform the Forward Pass**:\n",
        "    With the learning rate set, you now feed the input data through your model to get its predictions. This is the forward pass.\n",
        "    *   `outputs = model(inputs)`\n",
        "\n",
        "4.  **Calculate the Loss**:\n",
        "    Once you have the model's output, you compute the loss. This value quantifies how well the model is performing on the current batch of data according to your objective.\n",
        "    *   `loss = criterion(outputs, targets)`\n",
        "\n",
        "5.  **Execute the Backward Pass**:\n",
        "    With the loss calculated, you call `.backward()` on the loss tensor. This triggers PyTorch's autograd engine to compute the gradient of the loss with respect to every learnable parameter in your model.\n",
        "    *   `loss.backward()`\n",
        "\n",
        "6.  **Update Model Weights**:\n",
        "    Now, you call the optimizer's `step()` method. The optimizer uses the gradients computed in the backward pass to update the model's weights.\n",
        "    *   `optimizer.step()`\n",
        "\n",
        "7.  **Zero Out Gradients**:\n",
        "    After the weights have been updated, it's time to reset the gradients for the next iteration. By calling `optimizer.zero_grad()`, you ensure that the gradients from the next batch are calculated from a clean slate.\n",
        "    *   `optimizer.zero_grad()`\n",
        "\n",
        "8.  **Log and Validate**:\n",
        "    You will want to log the loss and any other relevant metrics to monitor training progress. Additionally, you should run a **validation loop** at a fixed frequency (e.g., every 5 epochs) to check generalization.\n",
        "    *   **Key Rule for Validation**: This loop runs on a separate validation dataset. You perform only the forward pass to calculate metricsâ€”**no backpropagation or optimizer steps**. It's critical to wrap this phase in a `with torch.no_grad():` block to prevent gradient computation, which saves memory and speeds up the process.\n",
        "    *   `if epoch % val_interval == 0: validate(...)`\n",
        "\n",
        "9.  **Save Model Checkpoints**:\n",
        "    Finally, you need to persist your model's progress. A common strategy is to save a \"checkpoint\" (the model weights and optimizer state) at fixed intervals, such as every 10 or 50 epochs.\n",
        "    *   This ensures that if your training crashes or you want to analyze the training dynamics later, you have snapshots of the model at different stages.\n",
        "    *   `if epoch % save_interval == 0: torch.save(...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeLZBjbhW7an"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- 0. Setup: Custom Dummy Dataset ---\n",
        "class SimpleDataset(Dataset):\n",
        "    def __len__(self):\n",
        "        return 100 # Dummy size\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns dummy input (size 10) and dummy target (size 1)\n",
        "        return torch.randn(10), torch.randn(1)\n",
        "\n",
        "# Setup Environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = nn.Linear(10, 1).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "dataloader = DataLoader(SimpleDataset(), batch_size=10)\n",
        "\n",
        "# Settings\n",
        "num_epochs = 20\n",
        "val_interval = 5\n",
        "save_interval = 10\n",
        "\n",
        "# --- The Training Loop ---\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train() # Ensure model is in training mode\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        # 1. Fetch a Batch of Data\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # 2. Adjust Learning Rate (Optional)\n",
        "        if epoch == 15: # Example: Decay LR at epoch 15\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 0.001\n",
        "\n",
        "        # 3. Perform the Forward Pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # 4. Calculate the Loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # 5. Execute the Backward Pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 6. Update Model Weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # 7. Zero Out Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # 8. Log and Validate\n",
        "    print(f\"Epoch {epoch} | Training Loss: {loss.item():.4f}\")\n",
        "\n",
        "    if epoch % val_interval == 0:\n",
        "        model.eval() # Switch to eval mode\n",
        "        with torch.no_grad():\n",
        "            # Reuse dataloader as dummy validation set for brevity\n",
        "            val_loss = sum(criterion(model(x.to(device)), y.to(device)) for x, y in dataloader)\n",
        "            print(f\"--> Validation Loss: {val_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # 9. Save Model Checkpoints\n",
        "    if epoch % save_interval == 0:\n",
        "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pth\")\n",
        "        print(f\"--> Saved checkpoint at epoch {epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212Son0eSTeb"
      },
      "source": [
        "### **3.4 Exercise: Full Training Cycle in MNIST**\n",
        "\n",
        "In this exercise, you will synthesize what you have learned about Datasets, DataLoaders, and the Training Loop to train a Convolutional Neural Network (CNN) on the classic MNIST dataset.\n",
        "\n",
        "**Scenario**:\n",
        "You are building a digit recognition pipeline. You must fetch raw data, process it manually into tensors, and execute a training loop that monitors validation loss.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "#### **1. Data Preparation**\n",
        "*   **Source**: Load the data using the Hugging Face `datasets` library.\n",
        "    *   `from datasets import load_dataset`\n",
        "    *   `dataset = load_dataset(\"ylecun/mnist\")`\n",
        "*   **Splitting**: Use the **'train'** split for training and the **'test'** split as your **Validation Set**.\n",
        "*   **Manual Preprocessing (Inside your Dataset)**:\n",
        "    You are **not** allowed to use `torchvision.transforms`. You must implement the following logic manually within your Dataset's `__getitem__`:\n",
        "    1.  **Format**: Convert the PIL Image to a numpy array, then to a PyTorch Tensor (`torch.from_numpy`). Convert to `float32`.\n",
        "    2.  **Normalization**: Normalize pixel values to the range `[-1, 1]` using the formula: `(pixel_value / 255.0 - 0.5) / 0.5`.\n",
        "    3.  **Labels**: Convert the integer label (0-9) to a `torch.LongTensor` (required for `CrossEntropyLoss`).\n",
        "\n",
        "#### **2. Class: MNISTDataset**\n",
        "Define a custom class `MNISTDataset(torch.utils.data.Dataset)`.\n",
        "*   **Initialization**: Accept the specific Hugging Face dataset split (e.g., `dataset['train']`).\n",
        "*   **Logic**: Implement `__len__` and `__getitem__` to retrieve samples and apply the preprocessing steps above.\n",
        "\n",
        "#### **3. Model Architecture**\n",
        "The architecture is provided below. The forward pass handles the Convolution layers, Pooling, Flattening, and Linear layers.\n",
        "\n",
        "```python\n",
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128) # 9216 = 12*12*64\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1) # Flatten feature maps\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "#### **4. Execution & Logging**\n",
        "*   **Training limit**: Train for a maximum of **30 epochs**.\n",
        "*   **Goal**: Minimize the **Validation Loss**.\n",
        "*   **Values to Monitor**: Print the Training Loss and Validation Loss every epoch.\n",
        "*   **Checkpointing**: Implement logic to save the `model.state_dict()` at regular intervals (e.g., every 10 epochs) or when validation loss improves.\n",
        "\n",
        "**Hint**: Remember to move your model and input tensors to the GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfBzJNmxAWi1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- 1. Data Loading & Inspection ---\n",
        "print(\"Loading MNIST dataset from Hugging Face...\")\n",
        "dataset = load_dataset(\"ylecun/mnist\")\n",
        "\n",
        "# Define splits as per assignment instructions\n",
        "train_split = dataset['train']\n",
        "val_split = dataset['test']\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"Training Set Size:   {len(train_split)}\")\n",
        "print(f\"Validation Set Size: {len(val_split)}\")\n",
        "\n",
        "# Inspect a single sample to understand the structure\n",
        "sample_item = train_split[0]\n",
        "print(f\"\\nSample Item Structure (Index 0):\")\n",
        "print(f\"Keys: {list(sample_item.keys())}\")\n",
        "print(f\"Content - 'image': {sample_item['image']} (Type: {type(sample_item['image'])})\")\n",
        "print(f\"Content - 'label': {sample_item['label']} (Type: {type(sample_item['label'])})\")\n",
        "\n",
        "\n",
        "# --- 2. Model Definition ---\n",
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128) # 9216 = 12*12*64\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1) # Flatten feature maps\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- 3. Dataset Implementation ---\n",
        "# TODO: Define your custom 'MNISTDataset' class here.\n",
        "class MNISTDataset(Dataset):\n",
        "    pass\n",
        "\n",
        "\n",
        "# --- 4. Training Setup ---\n",
        "# TODO: Instantiate your custom MNISTDataset for both the training and validation splits.\n",
        "train_dataset = None\n",
        "val_dataset = None\n",
        "\n",
        "# TODO: Create DataLoaders for both datasets (remember shuffle=True for training).\n",
        "train_loader = None\n",
        "val_loader = None\n",
        "\n",
        "# TODO: Initialize the Model, Optimizer (e.g., SGD/Adam), and Loss Function (CrossEntropyLoss).\n",
        "model = None\n",
        "optimizer = None\n",
        "criterion = None\n",
        "\n",
        "# --- 5. The Training Loop ---\n",
        "# TODO: Implement the full training loop for up to 30 epochs.\n",
        "training_epochs = 30\n",
        "log_interval = 100\n",
        "val_interval = 5\n",
        "save_interval = 10\n",
        "\n",
        "for epoch_idx in range(training_epochs):\n",
        "    average_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # TODO: preprocess data and batch forward/backward\n",
        "\n",
        "        # log necessary details\n",
        "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
        "            print(f\"[Training Epoch {epoch_idx+1}/{training_epochs}] Step {batch_idx+1}/{len(train_loader)}\\tAverage_Loss {average_loss:.4f}\")\n",
        "            average_loss = 0.0\n",
        "\n",
        "    # validation\n",
        "    if (epoch_idx + 1) % log_interval == 0 or epoch_idx == training_epochs - 1:\n",
        "        validation_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            # TODO: preprocess data, batch forward and accumulate loss\n",
        "\n",
        "        validation_loss = validation_loss / len(val_loader)\n",
        "        print(f\"[Validation Epoch {epoch_idx+1}/{training_epochs}] Average_Loss {validation_loss:.4f}\")\n",
        "\n",
        "    # save\n",
        "    if (epoch_idx + 1) % save_interval == 0:\n",
        "        # TODO: try saving model and optimizer state_dict here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cTVv3WdSJfWZ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
